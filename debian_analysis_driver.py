import os
import stat
import time
import datetime
import glob
import argparse
import ntpath
import shutil
import subprocess
import angr
import pickle
import json
import hashlib
import logging
import multiprocessing

import analyses
import hash_func_search

logging.getLogger('angr').setLevel(logging.CRITICAL)
logging.getLogger('cle').setLevel(logging.CRITICAL)
logging.getLogger('archinfo').setLevel(logging.CRITICAL)
logging.getLogger('pyvex').setLevel(logging.CRITICAL)
logging.getLogger('pyvex.lifting.libvex').setLevel(logging.CRITICAL)
logging.getLogger('claripy').setLevel(logging.CRITICAL)

l = logging.getLogger(name=__name__)
l.setLevel('CRITICAL')

ELF_HEADER = b"\x7fELF"
TIME_FORMAT = "%H:%M:%S, %9A, %d-%m-%Y"

FILE_PATH = os.path.dirname(os.path.realpath(__file__))
WORKSPACE_DIR = str(os.path.join(FILE_PATH, '.workspace'))

TIMEOUT = 600

NUM_DEB_FILES = 86189 # obtained with: $ls -R /home/nweidema/debian_mirror/packages/ | grep -P '[.]deb' | wc -l 

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('root', help='the root directory from where to search for all the .deb files')
    args = parser.parse_args()
    root_dir = args.root
    break_limit = -1
    #analyze = analyses.analyze3
    #analyze = analyses.analyze4
    analyze = analyses.analyze_get_just_hash
    #get_hash_funcs = hash_func_search.get_glibc_hash_func
    get_hash_funcs = hash_func_search.get_stashed_hash_funcs
    #deb_filter = analyses.deb_filter
    elf_filter = analyses.elf_size_filter
    deb_filter = None
    #elf_filter = None
    run_analysis(root_dir=root_dir, analyze=analyze, get_hash_funcs=get_hash_funcs, deb_filter=deb_filter, elf_filter=elf_filter, break_limit=break_limit)

def run_analysis(root_dir, analyze, get_hash_funcs, deb_filter=None, elf_filter=None, break_limit=-1):
    global_start_time = time.localtime()
    print("Testing started on: {}".format(time.strftime(TIME_FORMAT, global_start_time)))

    clear_workspace()

    results_file_name = "results_{}".format(time.strftime("%Y%m%d_%0H%0M"))
    full_results_file_name = os.path.join(FILE_PATH, 'data', results_file_name)
    print("Results file: {}".format(full_results_file_name))
    with open(full_results_file_name, 'w', buffering=1) as fd_results:
        deb_counter, elf_counter, err_counter, hit_counter = 0, 0, 0, 0
        break_counter = 0
        for r, d, f in os.walk(root_dir):
            for full_deb_file_name in glob.glob(os.path.join(r, '*.deb')):
                deb_file_md5 = md5_file(full_deb_file_name)
                deb_file_name = ntpath.basename(full_deb_file_name)
                if deb_filter and not deb_filter(full_deb_file_name, deb_file_name, deb_file_md5):
                    continue
                workspace_deb_file = setup_workspace(full_deb_file_name, deb_file_name)
                extract_deb(workspace_deb_file)
                for full_elf_file_name in find_elfs():
                    elf_file_md5 = md5_file(full_elf_file_name)
                    elf_file_name = ntpath.basename(full_elf_file_name)
                    elf_file_size = os.stat(full_elf_file_name).st_size
                    if elf_filter and not elf_filter(full_elf_file_name, elf_file_name, elf_file_md5, elf_file_size):
                        continue
                    l.debug('Analyzing: {}'.format(full_elf_file_name))
                    result_dict = multiprocessing.Manager().dict()
                    #result_dict = analyze(full_elf_file_name, get_hash_funcs)
                    analyze_process = multiprocessing.Process(target=do_analyze, args=(analyze, full_elf_file_name, get_hash_funcs, result_dict))
                    #iprint("Starting analysis for: {}:{}".format(deb_file_name, elf_file_name))
                    analyze_process.start()
                    analyze_process.join(timeout=TIMEOUT)
                    if analyze_process.is_alive():
                        analyze_process.terminate()
                        analyze_process.join()
                        #print("Done1")
                        result_dict['err'] = "Timeout"
                    else:
                        #print("Done2")
                        pass
                    elf_counter += 1
                    time_str = "{:%d %b %Y %H:%M:%S}".format(datetime.datetime.now())
                    print("{}/{} debs, {} elfs, hits: {}, errs: {} | {}".format(deb_counter, NUM_DEB_FILES, elf_counter, hit_counter, err_counter, time_str), end='\r')
                    #print("{}/{} debs, {} elfs, hits: {}, errs: {} | {}".format(deb_counter, NUM_DEB_FILES, elf_counter, hit_counter, err_counter, time_str))
                    #fd_results.write("{}\n".format(pickle.dumps(((deb_file_name, deb_file_md5), (elf_file_name, elf_file_md5), result_dict))))
                    file_dict = {'deb':{}, 'elf':{}}
                    file_dict['deb']['deb_name'] = deb_file_name
                    file_dict['deb']['deb_md5'] = deb_file_md5
                    file_dict['elf']['elf_name'] = elf_file_name
                    file_dict['elf']['elf_md5'] = elf_file_md5
                    file_dict['elf']['elf_size'] = elf_file_size
                    fd_results.write("{}\n".format(json.dumps((file_dict.copy(), result_dict.copy()), default=json_serialiblify)))
                    if 'err' in result_dict:
                        err_counter += 1
                    elif 'analysis_metrics' in result_dict and 'hit' in result_dict['analysis_metrics'] and result_dict['analysis_metrics']['hit']:
                        hit_counter += 1
                clear_workspace()
                deb_counter += 1
                break_counter += 1
                if break_limit > 0 and break_counter >= break_limit:
                    break
            if break_limit > 0 and break_counter >= break_limit:
                break
    time_str = "{:%d %b %Y %H:%M:%S}".format(datetime.datetime.now())
    print("{}/{} debs, {} elfs, hits: {}, errs: {} | {}".format(deb_counter, NUM_DEB_FILES, elf_counter, hit_counter, err_counter, time_str))
    global_end_time = time.localtime()
    global_run_time = format_time_delta(global_start_time, global_end_time)
    print("Results saved to: {}".format(full_results_file_name))
    print("Total time: {}".format(global_run_time))

def do_analyze(analyze, full_elf_file_name, get_hash_funcs, return_result_dict):
    result_dict = analyze(full_elf_file_name, get_hash_funcs)
    for k, v in result_dict.items():
        return_result_dict[k] = v

def extract_deb(deb_file_path):
    # TODO dpkg assumes the user has this installed. Maybe it would be better to extract manually? Wikipedia tells how this file should be structured: https://en.wikipedia.org/wiki/Deb_(file_format)
    #p = subprocess.Popen(['ar', 'xv', deb_file_path], stdout=subprocess.PIPE, cwd=WORKSPACE_DIR)
    #p.wait()
    #p = subprocess.Popen(['unxz', 'data.tar.xz'], stdout=subprocess.PIPE, cwd=WORKSPACE_DIR)
    #p.wait()
    #p = subprocess.Popen(['tar', 'xvf', 'data.tar'], stdout=subprocess.PIPE, cwd=WORKSPACE_DIR)
    #p.wait()
    p = subprocess.Popen(['dpkg', '-x', deb_file_path, WORKSPACE_DIR], cwd=WORKSPACE_DIR)
    p.wait()
    # Giving myself read and write permissions to all files and read/write/traverse permissions to directories
    for root, dirs, files in os.walk(WORKSPACE_DIR):
        for d in dirs:
            full_dir_path = os.path.join(root, d)
            if not os.path.islink(full_dir_path):
                os.chmod(os.path.join(root, d), stat.S_IREAD | stat.S_IWRITE | stat.S_IEXEC)
        for f in files:
            full_file_path = os.path.join(root, f)
            if not os.path.islink(full_file_path):
                os.chmod(os.path.join(root, f), stat.S_IREAD | stat.S_IWRITE)

def find_elfs():
    for r, d, f in os.walk(WORKSPACE_DIR):
        for file_name in f:
            full_file_path = os.path.join(r, file_name)
            # We don't want to analyze anything that may be somewhere else
            if not os.path.islink(full_file_path):
                with open(full_file_path, 'rb') as fd:
                    header = fd.read(4)
                    if header == ELF_HEADER:
                        yield full_file_path

def setup_workspace(full_deb_file_name, file_name):
    os.mkdir(WORKSPACE_DIR)
    dest = os.path.join(WORKSPACE_DIR, file_name)
    shutil.copyfile(full_deb_file_name, dest)
    return dest


def clear_workspace():
    shutil.rmtree(WORKSPACE_DIR)

def save_results(results):
    file_name = "results_{}".format(time.strftime("%Y%m%d_%0H%0M"))
    file_path = os.path.join(FILE_PATH, 'data', file_name)
    with open(file_path, 'w') as fd:
        json.dump(results, fd)
        return file_path

def md5_file(file_name):
    # I got this from: https://stackoverflow.com/questions/3431825/generating-an-md5-checksum-of-a-file
    hash_md5 = hashlib.md5()
    with open(file_name, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

def format_time_delta(start_time, end_time, short=False):
        start_time_datetime = datetime.datetime.fromtimestamp(time.mktime(start_time))
        end_time_datetime = datetime.datetime.fromtimestamp(time.mktime(end_time))
        time_delta_datetime = end_time_datetime - start_time_datetime
        seconds = int(time_delta_datetime.seconds)
        days, seconds = divmod(seconds, 86400)
        hours, seconds = divmod(seconds, 3600)
        minutes, seconds = divmod(seconds, 60)
        if short:
                return "{0}:{1:02d}:{2:02d}:{3:02d}".format(days, hours, minutes, seconds)
        else:
                return "{0} days, {1} hours, {2:02d} minutes and {3:02d} seconds.".format(days, hours, minutes, seconds)

def json_serialiblify(obj):
    if isinstance(obj, set):
        return list(obj)
    else:
        raise TypeError
    

if __name__ == "__main__":
    main()
