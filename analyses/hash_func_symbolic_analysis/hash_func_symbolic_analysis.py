import sys
import os
import angr
import claripy
import datetime
import json
import networkx as nx

from binalyzer.analyzers.analysis import Analysis
from binalyzer.analyzers.analysis_results import AnalysisResults

import util.angr_util as angr_util
from util.milestone_logger import MilestoneLogger


from analyses.hash_func_use.hash_func_use_analysis import HashFuncUseAnalysis
from analyses.hash_func_use.hash_func_use_analysis import HashFuncUseAnalysisResults
from analyses.hash_func_discovery.hash_func_discovery_analysis import HashFuncDiscoveryAnalysis


class HashFuncSymbolicAnalysis(Analysis):

    def __init__(self, similarity_score_cutoff, cached_results_path: str=None, cached_hash_func_results_path: str=None):
        super().__init__(cached_results_path)
        '''
        Parameters:
            cached_results_path: A path to the file containing cached results for this analysis.
            cached_hash_func_results_path: A path to the file containing cached results for the hash function discovery analysis part of this analysis.
        '''
        self._similarity_score_cutoff = similarity_score_cutoff

        self._hash_func_discovery_analysis = HashFuncDiscoveryAnalysis(self._similarity_score_cutoff, cached_results_path=cached_hash_func_results_path)
        

    def results_constructor(self):
        return HashFuncSymbolicAnalysisResults

    def analyze(self, analysis_target, results):

        try:
            full_target_file_path = analysis_target.full_file_path
            target_file_name = analysis_target.file_name

            hash_func_discovery_analysis_results = self._hash_func_discovery_analysis.results_constructor()()
            hash_func_discovery_start_time = datetime.datetime.now()
            self._hash_func_discovery_analysis.get_cache_or_analyze(analysis_target, hash_func_discovery_analysis_results)
            hash_func_discovery_end_time = datetime.datetime.now()
            # Only update the start and end times if the results were not cached
            if hash_func_discovery_analysis_results.get_cached_from() is None:
                hash_func_discovery_analysis_results.set_start_time(hash_func_discovery_start_time)
                hash_func_discovery_analysis_results.set_end_time(hash_func_discovery_end_time)


            results.set_hash_funcs_discovery_analysis_results(hash_func_discovery_analysis_results)
            hash_funcs_to_analyze = hash_func_discovery_analysis_results.discovered_hash_funcs

            if len(hash_funcs_to_analyze) <= 0:
                return
            proj = angr.Project(full_target_file_path, auto_load_libs=False)
            cfg_fast = proj.analyses.CFGFast(normalize=True)
            self._setup_simprocs(proj)


            # Let's analyze the smaller ones first (they'l probably be faster)
            hash_funcs_to_analyze = sorted(hash_funcs_to_analyze, key=lambda x: x.func_num_blocks)
            results.set_hash_funcs_to_analyze(hash_funcs_to_analyze)
            for discovered_hash_func in hash_funcs_to_analyze:
                hash_func_symbolic_analysis_details = HashFuncSymbolicAnalysisDetails(discovered_hash_func)
                idx = results.add_hash_func_symbolic_analysis_details(hash_func_symbolic_analysis_details)
                func_addr = discovered_hash_func.func_addr
                largest_isomorphic_subgraph_node_addrs = set(discovered_hash_func.largest_isomorphic_subgraph_node_addrs)

                hash_func = cfg_fast.functions.function(addr=func_addr)
                hash_alg_name = discovered_hash_func.hash_func_name

                if hash_func is None:
                    node = cfg_fast.model.get_any_node(func_addr, anyaddr=True)
                    if node is not None:
                        hash_func = cfg_fast.functions.function(node.function_address)
                if hash_func is None:
                    hash_func_symbolic_analysis_details.add_err("Could not find function at addr 0x{:x} (aborting further analysis).".format(func_addr))
                    results.update_hash_func_symbolic_analysis_details(idx, hash_func_symbolic_analysis_details)
                    continue
                print("hash func edges: {}".format(hash_func.graph.edges()))
                print("{}@0x{:x} ({})".format(hash_func.name, hash_func.addr, hash_alg_name))
                hash_func_symbolic_analysis_details.log_milestone('symbolic analysis of hash func started')
                results.update_hash_func_symbolic_analysis_details(idx, hash_func_symbolic_analysis_details)
                
                subgraph_nodes = []
                for addr in largest_isomorphic_subgraph_node_addrs:
                    node = cfg_fast.model.get_any_node(addr)
                    assert node is not None, "Could not find node for addr: 0x{:x}".format(addr)
                    subgraph_nodes.append(angr.codenode.BlockNode(node.addr, node.size))
                subgraph = hash_func.graph.subgraph(subgraph_nodes)
                # We want to choose a node to start symex in
                subgraph_sccs = list(nx.strongly_connected_components(subgraph))
                #print(list(subgraph_sccs))
                # We create a DAG from the SCCs, to topological sort and get the first nodes
                subgraph_condensed = nx.condensation(subgraph)
                first_scc = next(nx.topological_sort(subgraph_condensed))
                first_nodes = list(subgraph_condensed.nodes[first_scc]['members'])
                # If there is more than one node in the topologically first SCC, we arbitrarily choose the one with the smallest addr.
                # TODO, I suspect only one of these nodes will have incoming edges from outside the SCC. That should be the first one
                if len(first_nodes) > 1:
                    first_nodes = sorted(first_nodes, key=lambda n: n.addr)
                first_node = first_nodes[0]

                #start_state = proj.factory.blank_state(addr=first_node.addr, add_options=list(angr.sim_options.refs) + [angr.sim_options.ABSTRACT_MEMORY])
                start_state = proj.factory.blank_state(addr=first_node.addr, add_options=list(angr.sim_options.refs) + [angr.sim_options.NO_CROSS_INSN_OPT, angr.sim_options.MEMORY_CHUNK_INDIVIDUAL_READS], remove_options=[angr.sim_options.OPTIMIZE_IR])
                print("largest isomorphic subgraph node addrs:")
                print(largest_isomorphic_subgraph_node_addrs)
                def avoid(state):
                    
                    if state.addr in largest_isomorphic_subgraph_node_addrs:
                        return False
                    state_node = cfg_fast.model.get_any_node(state.addr, anyaddr=True)
                    if state_node.is_simprocedure:
                        return False # Do not avoid simprocedures
                    func = cfg_fast.functions.function(state_node.function_address)
                    if func.addr != hash_func.addr:
                        if func.is_plt:
                            return False # Do not avoid PLT calls
                        else:
                            return True
                    return state_node.addr not in largest_isomorphic_subgraph_node_addrs
                def b_mem_reads(state):
                    print(state.ip, state.inspect.mem_read_address, state.inspect.mem_read_expr)
                #start_state.inspect.b('mem_read', when=angr.BP_AFTER, action=b_mem_reads)

                simgr = proj.factory.simulation_manager(start_state)
                #simgr.run(n=len(subgraph_nodes))
                # We want to iterate the scc a couple more times to ensure we get more than one read
                extra_iterations = 1
                sccs_block_addrs = [set(block_node.addr for block_node in scc) for scc in subgraph_sccs if len((subgraph.subgraph(scc)).edges()) > 0]
                num_scc_blocks = sum(len(scc) for scc in subgraph_sccs if len((subgraph.subgraph(scc)).edges()) > 0)
                print("Num scc blocks: {}".format(num_scc_blocks))
                print(sccs_block_addrs)


                    
                    
                    

                
                simgr.explore(find=None, avoid=avoid, n=len(subgraph_nodes) + extra_iterations * num_scc_blocks)
                print('check2: {} {}'.format(simgr.stashes, simgr.errored))


                #print(simgr.stashes)
                for state in simgr.active:

                    var_to_offsets = {}
                    var_to_ips = {}
                    print(state)
                    #print('rax: {}'.format(set(state.regs.rax.variables)))
                    #print(state.solver.constraints)
                    for action in state.history.actions:
                        if isinstance(action, angr.state_plugins.SimActionData) and action.type == angr.state_plugins.SimActionData.MEM and action.action == angr.state_plugins.SimActionData.READ:
                            #print(action)
                            #print(action.addr, list(action.addr.leaf_asts()), action.data)
                            for var in action.addr.leaf_asts():
                                var_name = var.args[0] # there doesn't appear to be a better way to get the variable name of an AST
                                if var_name in action.addr.variables:
                                    #print("addr: {} (len: {}) var: {} (len: {})".format(action.addr, action.addr.length, var, var.length))
                                    #if var.length < action.addr.length:
                                    #    var = var.zero_extend(action.addr.length - var.length)
                                    #    #print("addr: {} (len: {}) var: {} (len: {})".format(action.addr, action.addr.length, var, var.length))
                                    offset = action.addr.replace(var, claripy.BVV(0, var.length))
                                    if not offset.symbolic:
                                        if var_name not in var_to_offsets:
                                            var_to_offsets[var_name] = []
                                            var_to_ips[var_name] = []
                                        print("0x{:x}: {}".format(action.ins_addr, var_name))
                                        var_to_offsets[var_name].append((state.solver.eval(offset), action.data))
                                        var_to_ips[var_name].append(action.ins_addr)
                    print("Non symbolic offsets:")
                    for var, offset_datas in var_to_offsets.items():
                        offsets = [offset for offset, data in offset_datas]
                        print(var, offsets)
                    # find buff read offsets:
                    print("Buf reads:")
                    buf_read_offsets = {}
                    buf_read_datas = []
                    buf_read_ips = {}
                    for var, offset_datas in var_to_offsets.items():
                        offsets = [offset for offset, data in offset_datas]
                        if min(offsets) != 0:
                            continue
                        # Make sure the memory addresses are sequential not necessarily in order, though
                        if len(set(offsets)) != max(offsets) + 1:
                            continue

                        if len(set(offsets)) <= extra_iterations:
                            continue
                        datas = [data for offset, data in offset_datas]
                        buf_read_offsets[var] = offsets
                        print(var, offsets)
                        #data_var_names = {var_name for data in datas for var_name in data.variables}
                        #print(data_var_names)
                        #for data in datas:
                            #for ast in data.leaf_asts():
                            #    state.solver.add(ast == ord('.'))
                            #    state.simplify()
                        buf_read_datas.append([ast for data in datas for ast in data.leaf_asts()])
                        buf_read_ips[var] = var_to_ips[var]
                    #for var, offset_datas in var_to_offsets.items():
                    #    var_name = var.args[0] # there doesn't appear to be a better way to get the variable name of an AST

                    # Second run:
                    print("Buf read breaks:")
                    start_state = proj.factory.blank_state(addr=first_node.addr, add_options=list(angr.sim_options.refs) + [angr.sim_options.NO_CROSS_INSN_OPT, angr.sim_options.MEMORY_CHUNK_INDIVIDUAL_READS], remove_options=[angr.sim_options.OPTIMIZE_IR])
                    def b_mem_read_constrain(state):
                        mem_read_addr = state.inspect.mem_read_address
                        for var in mem_read_addr.leaf_asts():
                            #print(list(var.leaf_asts()), buf_read_offsets)
                            #if any(ast.args[0] in buf_read_offsets for ast in var.leaf_asts()):
                            if any(state.addr in ips for ips in buf_read_ips.values()):
                                #print(state.ip, state.inspect.mem_read_address, state.inspect.mem_read_expr)
                                #state.solver.add(state.inspect.mem_read_expr == ord('.'))
                                print('constraining: {}'.format(state.inspect.mem_read_address))
                                #state.memory.store(state.inspect.mem_read_address, claripy.BVV(ord('.'), size=state.inspect.mem_read_length))
                                state.memory.store(state.inspect.mem_read_address, b'.')
                    #start_state.inspect.b('mem_read', when=angr.BP_AFTER, action=b_mem_read_constrain)
                    start_state.inspect.b('mem_read', when=angr.BP_BEFORE, action=b_mem_read_constrain)

                    print("start state: {}".format(start_state))
                    print("largest isomorphic subgraph node addrs:")
                    print([hex(addr) for addr in largest_isomorphic_subgraph_node_addrs])
                    simgr = proj.factory.simulation_manager(start_state)
                    simgr.explore(find=None, avoid=avoid, n=len(subgraph_nodes) + extra_iterations * num_scc_blocks)

                    print("action writes:")
                    for state in simgr.active:
                        block_addr = cfg_fast.model.get_any_node(addr=state.addr, anyaddr=True).addr
                        # We only want to look at those states after the scc
                        #print(list(scc_block_addrs for scc_block_addrs in sccs_block_addrs))
                        #if any(block_addr in scc_block_addrs for scc_block_addrs in sccs_block_addrs):
                        #    continue
                        print(state, block_addr, state.solver.satisfiable())
                        #print('rbp: {}'.format(state.solver.eval(state.regs.rbp)))
                        for action in state.history.recent_actions:
                            if isinstance(action, angr.state_plugins.SimActionData) and action.action != angr.state_plugins.SimActionData.READ:
                                if action.type == 'tmp':
                                    continue
                                if action.type == 'reg':
                                    reg_name = proj.arch.translate_register_name(action.offset)
                                    if reg_name.endswith('ip'):
                                        continue
                                    reg = proj.arch.get_register_by_name(reg_name)
                                    if not reg.general_purpose:
                                        continue
                                print("ip: 0x{:x} region: {} to: {} {} ({})".format(action.ins_addr, action.type, action.addr, action.offset, proj.arch.translate_register_name(action.offset)))
                                #print(action.data)
                                print("data: {} symbolic: {}".format(state.solver.eval(action.data), action.data.symbolic))
                                print("data vars: {}".format(set(ast.args[0] for ast in action.data.leaf_asts())))

                        
                   
                        
                    #print(len(list(state.history.lineage)))
                    '''
                    print("Writes:")
                    for action in state.history.recent_actions:
                        if isinstance(action, angr.state_plugins.SimActionData) and action.action != angr.state_plugins.SimActionData.READ:
                            if action.type == 'tmp':
                                continue
                            #data_operations = set([ast.op for ast in action.data.children_asts()])
                            #if len(data_operations) < 3:
                            #    continue
                            #print("region: {} to: {} {} ({}) data ops: {}".format(action.type, action.addr, action.offset, proj.arch.translate_register_name(action.offset), data_operations))
                            print("region: {} to: {} {} ({})".format(action.type, action.addr, action.offset, proj.arch.translate_register_name(action.offset)))
                            data = action.data
                            data_var_names = {var_name for var_name in data.variables}
                            data_sym_var_names = set()
                            for var in data.leaf_asts():
                                var_name = var.args[0]
                                if var.symbolic:
                                    data_sym_var_names.add(var_name)
                            #print(data_var_names)
                            print("data_sym_var_names: {}".format(data_sym_var_names))
                            #print("data_var_names: {}".format(data_var_names))
                            for buff_read_data in buf_read_datas:
                                buf_read_data_var_names = set(var.args[0] for var in buff_read_data)
                                print("buff_read_data_var_names: {}".format(buf_read_data_var_names))
                                # If the symbolic expression of the data we are writing contains all the buffer read variables
                                if len(buf_read_data_var_names.intersection(data_sym_var_names)) >= len(buf_read_data_var_names):
                                #if len(buf_read_data_var_names.intersection(data_var_names)) > 0:
                                    print("Buffer computation intersection:")
                                    #leaf_asts = list(data.leaf_asts())
                                    #exe_data = data.copy()
                                    #for ast in leaf_asts:
                                    #    if ast.args[0] in data_sym_var_names:
                                    #        exe_data = exe_data.replace(ast, claripy.BVV(ord('.'), ast.length))
                                    #print(exe_data)

                                    #print('eval\'ed: {}'.format(state.solver.eval(exe_data)))
                                    extra_constraints = [var == ord('a') for var in buff_read_data if var.symbolic]
                                    if state.solver.satisfiable(extra_constraints=extra_constraints):
                                        sol = state.solver.eval(data, extra_constraints=extra_constraints)
                                        print('eval\'ed: {}'.format(sol))
                                    else:
                                        print("(unsat)")
                                elif len(data_sym_var_names) > extra_iterations and len(buf_read_data_var_names) - 1 <= len(data_sym_var_names) <= len(buf_read_data_var_names): # We execute the entire subgraph once + extra iterations (extra iterations consists of the number of blocks in scc, so we should find a data var with each iteration)
                                    #data_operations = set([ast.op for ast in action.data.children_asts()])
                                    #print(data_operations)
                                    #if len(data_operations) > 4:
                                    print("Buffer computation length:")
                                    #print(data)
                                    #leaf_asts = list(data.leaf_asts())
                                    #exe_data = data.copy()
                                    #for ast in leaf_asts:
                                    #    if ast.args[0] in data_sym_var_names:
                                    #        exe_data = exe_data.replace(ast, claripy.BVV(ord('.'), ast.length))

                                    #print('eval\'ed: {}'.format(state.solver.eval(exe_data)))
                                    extra_constraints = [var == ord('.') for var in buff_read_data if var.symbolic]
                                    if state.solver.satisfiable(extra_constraints=extra_constraints):
                                        sol = state.solver.eval(data, extra_constraints=extra_constraints)
                                        print('eval\'ed: {}'.format(sol))
                                    else:
                                        print("(unsat)")
                    '''
                                  







        except Exception as e:
            raise e
            results.add_err(str(e))

    def perform_static_inline_filters(self, discovered_hash_funcs, graph_order_diff_limit):
        lonely_hash_funcs = self.find_lonely_hash_funcs(discovered_hash_funcs)
        passed_hash_funcs = []
        if len(lonely_hash_funcs) > 0:
            passed_hash_funcs = self.filter_hash_funcs_graph_order(lonely_hash_funcs, graph_order_diff_limit)
        return passed_hash_funcs

    def _setup_simprocs(self, proj):
        class MyToLowerSimProc(angr.SimProcedure):
            def run(self):
                if self.state.libc.ctype_tolower_loc_table_ptr is None:
                    malloc = angr.SIM_PROCEDURES['libc']['malloc']
                    # 384 entries, 4 bytes each
                    table = self.inline_call(malloc, 384*4).ret_expr
                    table_ptr = self.inline_call(malloc, self.state.arch.bytes).ret_expr

                    for pos, c in enumerate(self.state.libc.TOLOWER_LOC_ARRAY):
                        self.state.memory.store(table + (pos * 4),
                                                self.state.solver.BVV(c, 32),
                                                endness=self.state.arch.memory_endness,
                                                inspect=False,
                                                disable_actions=True,
                                                )

                    # Offset for negative chars: -128 index (4 bytes per index)
                    table += (128 * 4)
                    self.state.memory.store(table_ptr,
                                            table,
                                            size=self.state.arch.bytes,
                                            endness=self.state.arch.memory_endness,
                                            inspect=False,
                                            disable_actions=True,
                                            )

                    self.state.libc.ctype_tolower_loc_table_ptr = table_ptr
                return self.state.libc.ctype_tolower_loc_table_ptr
        proj.hook_symbol('__ctype_tolower_loc', MyToLowerSimProc())
        class MyToUpperSimProc(angr.SimProcedure):
            def run(self):
                if self.state.libc.ctype_toupper_loc_table_ptr is None:
                    malloc = angr.SIM_PROCEDURES['libc']['malloc']
                    # 384 entries, 4 bytes each
                    table = self.inline_call(malloc, 384*4).ret_expr
                    table_ptr = self.inline_call(malloc, self.state.arch.bytes).ret_expr

                    for pos, c in enumerate(self.state.libc.TOUPPER_LOC_ARRAY):
                        self.state.memory.store(table + (pos * 4),
                                                self.state.solver.BVV(c, 32),
                                                endness=self.state.arch.memory_endness,
                                                inspect=False,
                                                disable_actions=True,
                                                )

                    # Offset for negative chars: -128 index (4 bytes per index)
                    table += (128 * 4)
                    self.state.memory.store(table_ptr,
                                            table,
                                            size=self.state.arch.bytes,
                                            endness=self.state.arch.memory_endness,
                                            inspect=False,
                                            disable_actions=True,
                                            )

                    self.state.libc.ctype_toupper_loc_table_ptr = table_ptr
                return self.state.libc.ctype_toupper_loc_table_ptr
        proj.hook_symbol('__ctype_toupper_loc', MyToLowerSimProc())
        

class HashFuncSymbolicAnalysisResults(AnalysisResults,MilestoneLogger):

    def __init__(self):
        MilestoneLogger.__init__(self)
        AnalysisResults.__init__(self)
        self.hash_funcs_discovery_analysis_results = None
        self.hash_funcs_to_analyze = None
        self.hash_func_symbolic_analysis_details_list = []
        self.errs = []

    def set_hash_funcs_discovery_analysis_results(self, value):
        self.hash_funcs_discovery_analysis_results = value

    def set_hash_funcs_to_analyze(self, value):
        self.hash_funcs_to_analyze = value

    def add_hash_func_symbolic_analysis_details(self, hash_func_symbolic_analysis_details):
        self.hash_func_symbolic_analysis_details_list.append(hash_func_symbolic_analysis_details)
        return len(self.hash_func_symbolic_analysis_details_list) - 1

    def update_hash_func_symbolic_analysis_details(self, i, hash_func_symbolic_analysis_details):
        self.hash_func_symbolic_analysis_details_list[i] = hash_func_symbolic_analysis_details

    def get_tracked_events(self):
        tracked_events = {}
        tracked_events['errs'] = len(self.errs)
        tracked_events['timeouts'] = len([e for e in self.errs if 'timeout' in e])

        return tracked_events

    def copy_from_inner(self, other_analysis_results):
        self.set_hash_funcs_discovery_analysis_results(other_analysis_results.hash_funcs_discovery_analysis_results)
        for hash_func_symbolic_analysis_details in other_analysis_results.hash_func_symbolic_analysis_details_list:
            self.add_hash_func_symbolic_analysis_details(hash_func_symbolic_analysis_details)
        for milestone_timestamp in other_analysis_results.milestone_timestamps:
            milestone_msg, milestone_time = milestone_timestamp
            self.add_milestone(milestone_msg, milestone_time)
        for err in other_analysis_results.errs:
            self.add_err(err)

class HashFuncSymbolicAnalysisDetails(MilestoneLogger):

    # analysis_target_hash_func should be  discovered hash func
    def __init__(self, analysis_target_hash_func):
        MilestoneLogger.__init__(self)
        self.analysis_target_hash_func = analysis_target_hash_func

        self.milestone_timestamps = []

        self.errs = []

    def add_err(self, err):
        self.errs.append(err)

