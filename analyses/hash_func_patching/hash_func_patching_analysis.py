import sys
import os
import angr
import claripy
import datetime
import json

from binalyzer.analyzers.analysis import Analysis
from binalyzer.analyzers.analysis_results import AnalysisResults

import util.angr_util as angr_util
from util.milestone_logger import MilestoneLogger

from hash_patcher.hash_patch_evaluator import HashPatchEvaluator

from hash_patcher.static_patcher import StaticPatcher
from hash_patcher.hash_patch_asm_generator import HashPatchAsmGenerator

from analyses.hash_func_use.hash_func_use_analysis import HashFuncUseAnalysis
from analyses.hash_func_use.hash_func_use_analysis import HashFuncUseAnalysisResults
from analyses.hash_func_discovery.hash_func_discovery_analysis import HashFuncDiscoveryAnalysis

symb_ret_check_strs = ['a', 'ab', 'abc']
case_sensitivity_check_strs = ['Ab', 'bD', '0sY']

# Note we only use strings that are not affected by case-sensitivity
hash_vals_checks = {
    'bkdrhash':{
        '123':{847490},
        '!@#$':{75289928},
	'\x24\x60\x5b\x3a\x7b\x5b': {4294967291, 1417339207675},
        '\x3c\x2c\x40\x3b\x40\x7c': {4294967279, 2327872274415},
        '\x36\x2a\x7d\x5c\x5e\x2e': {4294967231, 2095944040383},
        '\x3d\x3b\x2b\x35\x2c\x21': {4294967197, 2370821947293},
        '\x24\x26\x2f\x3d\x34\x2d': {4294967189, 1400159338389}
    },
    'dekhash':{
        '123':{82547},
        '!@#$':{5342276},
	'\x3c\x7c\x7c\x7c\x7e\x3a': {4294967291, 8589934587},
        '\x3c\x7c\x7c\x7c\x7e\x2e': {4294967279, 8589934575},
        '\x3c\x7c\x7c\x7c\x7c\x3e': {4294967231, 8589934527},
        '\x3c\x7c\x7c\x7c\x7d\x34': {4294967197, 8589934485},
        '\x3c\x7c\x7c\x7c\x7d\x3c': {4294967189, 8589934493}
    },
    'pjwhash':{
        '123':{13395},
        '!@#$':{152148},
    },
    'rshash':{
        '123':{3350397308, 17783548483143401852}, # 32-bit, 64-bit
        '!@#$':{3818697610, 14295925813946397578}, # 32-bit, 64-bit
        '\x33\x2e\x2d\x28\x3b\x2e': {4294967291, 11919897670883213307},
        '\x7c\x29\x2a\x2a\x3b\x3d': {4294967279, 15106615636165394415},
        '\x3c\x39\x2f\x3c\x24\x7b': {4294967231, 12739185140979204031},
        '\x3b\x2f\x28\x2a\x5c\x21\x7e': {4294967197, 9006589687212539805},
        '\x34\x7e\x25\x3e\x2f\x3b': {4294967189, 13904908104849424277}
    },
    'jshash':{
        '123':{446351968, 43675163478624}, # 32-bit, 64-bit
        '!@#$':{1131574688, 1449926860109216}, # 32-bit, 64-bit
        '\x2c\x36\x35\x39\x7d\x60': {4294967291, 1453132520697626619},
        '\x2c\x36\x35\x37\x3a\x32': {4294967279, 1453132520697626607},
        '\x2c\x36\x35\x37\x38\x40': {4294967231, 1453132520697626559},
        '\x3f\x5b\x7d\x40\x7e\x26\x2d': {4294967197, 11418380502618341277},
        '\x2c\x36\x35\x37\x37\x39': {4294967189, 1453132520697626517}
    },
    'sdbmhash':{
        '123':{408093746, 210861491250}, # 32-bit, 64-bit
        '!@#$':{520606112, 9315763110793632}, # 32-bit, 64-bit
        '\x3e\x27\x38\x36\x29\x7d': {4294967291, 13570763544755961851},
        '\x3e\x40\x5b\x39\x2b\x3a': {4294967279, 15356494621538516975},
        '\x2e\x5c\x24\x20\x5d\x32': {4294967231, 4430835678716100543},
        '\x21\x32\x40\x7e\x5d\x2b': {4294967197, 3656939510358343581},
        '\x21\x32\x40\x7e\x5d\x23': {4294967189, 3656939510358343573}
    },
    'fnvhash':{
        '123':{1916298011, 15625645075545482011}, # 32-bit, 64-bit
        '!@#$':{1436240147, 6581361711186135315}, # 32-bit, 64-bit
        '\x38\x34\x24\x27\x37\x5c': {4294967291, 17078418547480199163},
        '\x7e\x5f\x25\x35\x2e\x27': {4294967279, 15677458262008004591},
        '\x38\x24\x30\x3e\x33\x31': {4294967231, 16177970167018422207},
        '\x37\x24\x3a\x33\x3d\x37': {4294967197, 11576875881540353949},
        '\x28\x3e\x7e\x30\x37\x39': {4294967189, 10516239896841027477}
    },
    'djbhash':{
        '123':{193432059, 193432059, 193359061, 193359061}, #DJBX33A-32, DJBX33A-64, DJBX33X-32, DJBX33X-64
        '!@#$':{2087730413, 6382697709, 2085358563, 6380325859},
        '\x2a\x26\x23\x25\x2b\x40\x33': {4294967291, 229385613344763, 192811223, 229299906810071}, #DJBX33A,DJBX33X
        '\x7d\x2a\x38\x3e\x27\x5e\x28': {4294967279, 229492987527151, 2474805029, 229396678084389},
        '\x2a\x26\x23\x25\x2b\x3e\x39': {4294967231, 229385613344703, 192808803, 229299906807651},
        '\x5c\x22\x29\x34\x2a\x21\x32': {4294967197, 229450037854109, 2528354975, 229439681307295},
        '\x20\x25\x5d\x28\x40\x22\x24': {4294967189, 229372728442773, 346362355, 229287175459315},
        '\x30\x2b\x2d\x23\x40\x3e\x35': {3661327619, 229393569639683, 4294967291, 229308303933435},
        '\x5e\x31\x29\x2c\x2a\x31\x7b': {3169679839, 229453207534047, 4294967279, 229441447919599},
        '\x5e\x31\x29\x2c\x2a\x31\x2b': {3169679759, 229453207533967, 4294967231, 229441447919551},
        '\x5e\x31\x29\x2c\x2a\x30\x28': {3169679723, 229453207533931, 4294967197, 229441447919517},
        '\x3c\x34\x36\x32\x2a\x2a\x3c': {2342480333, 229409430661581, 4294967189, 229312598900629}
    }, # We allow for both DJBX33A and DJBX33X
    'elfhash':{
        '123':{13395},
        '!@#$':{152148},
        '\x5d\x2a\x5d\x28\x7d\x28\x2f': {268435455},
        '\x2d\x2a\x5d\x28\x7c\x36\x7e': {268435454},
        '\x3d\x2d\x2c\x3d\x2d\x27\x5d': {268435453},
        '\x28\x7c\x3c\x38\x7c\x3b\x2b': {268435451}
    },
    'aphash':{
        '123':{1654819135, 6821471536116042047}, # 32-bit, 64-bit
        '!@#$':{4030010601, 18423287767860319465},
        '\x30\x27\x3e\x2c\x35\x39': {4294967291, 13245871185226440699},
        '\x35\x35\x28\x35\x2a\x24': {4294967279, 11825083993558941679},
        '\x35\x3c\x35\x27\x5f\x34': {4294967231, 5471184156792936383},
        '\x30\x27\x3e\x2c\x35\x5f': {4294967197, 13245871185226440605},
        '\x35\x35\x28\x35\x2a\x5e': {4294967189, 11825083993558941589}
    }
}

# Some hash algorithms cannot output all int values (like elf hash and pjw hash)
# I should probably check if any of the other hashes are like this too
hash_to_out_mod = {'bkdrhash':2**32,
    'dekhash':2**32,
    'pjwhash':2**28,
    'rshash':2**32,
    'jshash':2**32,
    'sdbmhash':2**32,
    'fnvhash':2**32,
    'djbhash':2**32,
    'elfhash':2**28,
    'aphash':2**32
}

ext_mem_refs_checks = ['aa', 'abcd']

hash_patches_location = os.path.join(os.path.dirname(os.path.realpath(__file__)), '..', '..', 'hash_patches')
case_sensitive_hash_patch_asm_files = [os.path.join(hash_patches_location, 'sip_hash_func.s'),
    os.path.join(hash_patches_location, 'univ_hash_func.s')
]
case_insensitive_hash_patch_asm_files = [os.path.join(hash_patches_location, 'sip_hash_func_case_insensitive.s'),
    os.path.join(hash_patches_location, 'univ_hash_func_case_insensitive.s')
]


class HashFuncPatchingAnalysis(Analysis):

    def __init__(self, similarity_score_cutoff, block_symex_upperbound, restrict_hash_tables, graph_order_diff_limit, patch_dir, enable_static_inlined_hash_func_filters=True, cached_results_path: str=None, cached_hash_func_results_path: str=None):
        super().__init__(cached_results_path)
        '''
        Parameters:
            similarity_score_cutoff: The score at which we consider a classification to be correct (deprecated)
            block_symex_upperbound: When performing symbolic execution on the return value of a hash function (to check for reads/writes indicating a hash table) this is the max number of blocks we will execute.
            restrict_hash_tables: If true, we will perform symbolic execution on the return value of a hash function to see if we can find reads/writes to an address depending on the hash value. This indicates use of a hash table.
            graph_order_diff_limit: The difference we allow in the order between a detected hash function's CFG and the passed SCC. This is used to filter out inlined hash functions. (None to disable limit)
            patch_dir: The directory where the patched binaris will be stored.
            enable_static_inlined_hash_func_filters: Use a heuristic static analysis to remove hash functions that appear to have been inlined from the set of statically discovered hash functions.
            cached_results_path: A path to the file containing cached results for this analysis.
            cached_hash_func_results_path: A path to the file containing cached results for the hash function discovery analysis part of this analysis.
        '''
        self._similarity_score_cutoff = similarity_score_cutoff
        self._block_symex_upperbound = block_symex_upperbound
        self._graph_order_diff_limit = graph_order_diff_limit
        self._enable_static_inlined_hash_func_filters = enable_static_inlined_hash_func_filters
        self._patch_dir = patch_dir
        self._restrict_hash_tables = restrict_hash_tables

        self._hash_func_discovery_analysis = HashFuncDiscoveryAnalysis(self._similarity_score_cutoff, cached_results_path=cached_hash_func_results_path)
        self._hash_func_use_analysis = HashFuncUseAnalysis(self._similarity_score_cutoff, self._block_symex_upperbound)
        self._hash_vals_checks = self.load_hash_vals_checks()

    def results_constructor(self):
        return HashFuncPatchingAnalysisResults

    def analyze(self, analysis_target, results):
        results.log_milestone('patching analysis started')

        try:
            full_target_file_path = analysis_target.full_file_path
            target_file_name = analysis_target.file_name

            hash_func_discovery_analysis_results = self._hash_func_discovery_analysis.results_constructor()()
            hash_func_discovery_start_time = datetime.datetime.now()
            self._hash_func_discovery_analysis.get_cache_or_analyze(analysis_target, hash_func_discovery_analysis_results)
            hash_func_discovery_end_time = datetime.datetime.now()
            # Only update the start and end times if the results were not cached
            if hash_func_discovery_analysis_results.get_cached_from() is None:
                hash_func_discovery_analysis_results.set_start_time(hash_func_discovery_start_time)
                hash_func_discovery_analysis_results.set_end_time(hash_func_discovery_end_time)


            # If we only want to patch hash functions used with hash tables
            # We filter out all hash other functions
            hash_func_use_analysis_results = None
            if self._restrict_hash_tables:
                hash_func_use_analysis_results = self._hash_func_use_analysis.results_constructor()()
                hash_func_use_start_time = datetime.datetime.now()
                self._hash_func_use_analysis.get_cache_or_analyze(analysis_target, hash_func_use_analysis_results)
                hash_func_use_end_time = datetime.datetime.now()
                # Only update the start and end times if the results were not cached
                if hash_func_use_analysis_results.get_cached_from() is None:
                    hash_func_use_analysis_results.set_start_time(hash_func_use_start_time)
                    hash_func_use_analysis_results.set_end_time(hash_func_use_end_time)
                hash_func_uses = hash_func_use_analysis_results.hash_func_uses
                # Filter in hash functions that appear to be used with hash tables
                hash_funcs_to_patch = set()
                # Go through all the hash function uses (hash function + call site + reads/writes/seeks)
                for hash_func_use in hash_func_uses:
                    # If the hash function is used, add it to a set
                    # (We need to use a set, because one hash function can have many call sites)
                    if self.hash_func_is_used(hash_func_use):
                        hash_func = hash_func_use.discovered_hash_func
                        hash_funcs_to_patch.add(hash_func)
            else:
                hash_funcs_to_patch = hash_func_discovery_analysis_results.discovered_hash_funcs

            if self._enable_static_inlined_hash_func_filters:
                # Perform static analysis to (try to) filter out inlined hash functions
                hash_funcs_to_patch = self.perform_static_inline_filters(hash_funcs_to_patch, self._graph_order_diff_limit)

            results.log_milestone('completed: obtain hash funcs to patch')
            assert hash_func_discovery_analysis_results is not None, "Logic error: by now we need to know what hash functions to patch"
            # patching only hash functions used with hash tables implies that by now we should have the hash func use analysis results
            assert (not self._restrict_hash_tables) or (hash_func_use_analysis_results is not None), "Logic error: we want to restrict to patching hash table functions, but we should have had the analysis results by now"
            results.set_hash_funcs_discovery_analysis_results(hash_func_discovery_analysis_results)
            results.set_hash_func_use_analysis_results(hash_func_use_analysis_results)
            if len(hash_funcs_to_patch) <= 0:
                return
            proj = angr.Project(full_target_file_path, auto_load_libs=False)
            cfg_fast = proj.analyses.CFGFast()


            # Let's analyze and try to patch the smaller ones first (they'l probably be faster)
            hash_funcs_to_patch = sorted(hash_funcs_to_patch, key=lambda x: x.func_num_blocks)
            results.set_hash_funcs_to_patch(hash_funcs_to_patch)
            results.log_milestone('started patching hash funcs')
            # For every used hash function patch it
            for discovered_hash_func in hash_funcs_to_patch:
                hash_func_patch_details = HashFuncPatchDetails(discovered_hash_func)
                idx = results.add_hash_func_patch_details(hash_func_patch_details)
                func_addr = discovered_hash_func.func_addr

                patched_elf_file_name = target_file_name + "_patch_0x{:x}".format(func_addr)
                patched_elf_file_path = os.path.join(self._patch_dir, patched_elf_file_name)
                hash_func = cfg_fast.functions.function(addr=func_addr)
                hash_alg_name = discovered_hash_func.hash_func_name

                if hash_func is None:
                    node = cfg_fast.model.get_any_node(func_addr, anyaddr=True)
                    if node is not None:
                        hash_func = cfg_fast.functions.function(node.function_address)
                if hash_func is None:
                    hash_func_patch_details.add_err("Could not find function at addr 0x{:x} (aborting patching).".format(func_addr))
                    results.update_hash_func_patch_details(idx, hash_func_patch_details)
                    continue
                hash_func_patch_details.log_milestone('patch evaluation started')
                results.update_hash_func_patch_details(idx, hash_func_patch_details)

                # We start with checks on the original hash function
                hash_patch_evaluator_sig1 = HashPatchEvaluator(full_target_file_path, func_addr, 'sig1', vuln_proj=proj)
                hash_patch_evaluator_sig2 = HashPatchEvaluator(full_target_file_path, func_addr, 'sig2', vuln_proj=proj)


                # Check the signature
                sig1_check = hash_patch_evaluator_sig1.check_signature(check_original=True)
                sig2_check = hash_patch_evaluator_sig2.check_signature(check_original=True)

                hash_func_patch_details.orig_hash_sig1 = sig1_check
                hash_func_patch_details.orig_hash_sig2 = sig2_check
                results.update_hash_func_patch_details(idx, hash_func_patch_details)

                if not (sig1_check or sig2_check):
                    hash_func_patch_details.add_err("Could not find a signature that works (aborting patching).")
                    results.update_hash_func_patch_details(idx, hash_func_patch_details)
                    continue

                # If both are true, we can probably pick either right?
                if sig1_check:
                    hash_patch_evaluator = hash_patch_evaluator_sig1
                else:
                    hash_patch_evaluator = hash_patch_evaluator_sig2
                hash_func_patch_details.log_milestone('completed: signature detection')
                results.update_hash_func_patch_details(idx, hash_func_patch_details)


                # Check the hash values
                symbolic_ret_irregularities = hash_patch_evaluator.check_symb_ret(symb_ret_check_strs, check_original=True)
                hash_func_patch_details.orig_hash_symb_ret_irregularities = symbolic_ret_irregularities
                results.update_hash_func_patch_details(idx, hash_func_patch_details)
                #str_to_hashval_map = hash_vals_checks[hash_alg_name]
                str_to_hashval_map = self._hash_vals_checks[hash_alg_name]
                # XXX This causes a problem for hash functions that cast down their return
                hash_val_irregularities = hash_patch_evaluator.check_hash_vals(hash_alg_name, str_to_hashval_map, check_original=True)
                hash_func_patch_details.orig_hash_val_irregularities = hash_val_irregularities
                hash_func_patch_details.log_milestone('completed: orig hash values check')
                results.update_hash_func_patch_details(idx, hash_func_patch_details)

                # Check for case sensitivity
                case_sensitivity = hash_patch_evaluator.check_case_sensitivity(case_sensitivity_check_strs, check_original=True)
                hash_func_patch_details.orig_case_sensitive = case_sensitivity
                hash_func_patch_details.log_milestone('completed: orig case sensitivity check')
                results.update_hash_func_patch_details(idx, hash_func_patch_details)


                # Check the external memory references (outside the stack-frame of the hash function)
                ext_mem_refs_irregularities = hash_patch_evaluator.check_ext_mem_refs(ext_mem_refs_checks, check_original=True)
                hash_func_patch_details.orig_hash_ext_mem_ref_irregularities = ext_mem_refs_irregularities
                hash_func_patch_details.log_milestone('completed: orig ext mem refs check')
                results.update_hash_func_patch_details(idx, hash_func_patch_details)


                # Now do the patch
                if case_sensitivity:
                    hash_patch_asm_files = case_sensitive_hash_patch_asm_files
                else:
                    hash_patch_asm_files = case_insensitive_hash_patch_asm_files
                if sig1_check:
                    len_in_reg = 'rsi'
                else:
                    len_in_reg = None
                # Get a value to mod the output of the replacement hash with
                out_mod = hash_to_out_mod[hash_alg_name]

                hash_func_patch_details.patch_insert_successes = {}
                results.update_hash_func_patch_details(idx, hash_func_patch_details)
                for hash_patch_asm_file in hash_patch_asm_files:
                    hash_func_patch_details.patch_insert_successes[os.path.basename(hash_patch_asm_file)] = False
                    results.update_hash_func_patch_details(idx, hash_func_patch_details)
                    if hash_patch_asm_file.startswith('sip_hash'):
                        key0 = random.randint(0, 2**64 - 1)
                        key1 = random.randint(0, 2**64 - 1)
                        random_vals = {'rval1': key0 ^ 0x6c7967656e657261, 'rval2': k1 ^ 0x646f72616e646f6d, 'rval3': k0 ^ 0x736f6d6570736575, 'rval4': k1 ^ 0x7465646279746573}
                    elif hash_patch_asm_file.startswith('univ_hash'):
                        rval1 = random.randint(0, 2**32 - 1)
                        rval2 = random.randint(0, 2**32 - 1)
                        random_vals = {'rval1': rval1, 'rval2': rval2}

                    else:

                        raise Exception("I don't know how to randomize algorithm in: {}".format(hash_path_asm_file))

                    hpag = HashPatchAsmGenerator(proj, asm_file=hash_patch_asm_file, random_vals=random_vals, buf_in_reg='rdi', len_in_reg=len_in_reg, out_mod=out_mod)
                    hash_patch_asm_code = hpag.get_asm_code()
                    patcher = StaticPatcher(full_target_file_path)
                    patch_success = patcher.hook_func(hash_func.addr, hash_patch_asm_code, patch_call_sites=False, patch_orig_func=True)


                    if patch_success:
                        patcher.apply_patches(patched_elf_file_path)
                        hash_func_patch_details.patch_insert_successes[os.path.basename(hash_patch_asm_file)] = True
                        results.update_hash_func_patch_details(idx, hash_func_patch_details)
                        break
                if not patch_success:
                    hash_func_patch_details.add_err("Could not add replacement hash function to binary (aborting patching).")
                    results.update_hash_func_patch_details(idx, hash_func_patch_details)
                    continue
                hash_func_patch_details.log_milestone('completed: patch inserted replacement hash func')
                results.update_hash_func_patch_details(idx, hash_func_patch_details)

                # Now check the patched hash function
                hash_patch_evaluator.set_patch(patched_elf_file_path)

                # Case sensitivity
                patch_case_sensitivity = hash_patch_evaluator.check_case_sensitivity(case_sensitivity_check_strs, check_original=False)
                hash_func_patch_details.patch_case_sensitive = patch_case_sensitivity
                hash_func_patch_details.log_milestone('completed: patch case sensitivity check')
                results.update_hash_func_patch_details(idx, hash_func_patch_details)

                # Check the external memory references (outside the stack-frame of the hash function)
                ext_mem_refs_irregularities = hash_patch_evaluator.check_ext_mem_refs(ext_mem_refs_checks, check_original=False) # We need to use the same concrete buffers so that we know what memory references to expect
                hash_func_patch_details.patch_hash_ext_mem_ref_irregularities = ext_mem_refs_irregularities
                hash_func_patch_details.log_milestone('completed: patch ext mem refs check')
                results.update_hash_func_patch_details(idx, hash_func_patch_details)


                # Check that hash values we get for the patched hash function have preimages for the original hash function
                range_irregularities = hash_patch_evaluator.check_hash_vals_range(conc_bufs=['aa', 'AbC'])
                hash_func_patch_details.range_irregularities = range_irregularities
                hash_func_patch_details.log_milestone('completed: patch hash val range check')
                results.update_hash_func_patch_details(idx, hash_func_patch_details)

                hash_func_patch_details.log_milestone('completed: patch evaluation finished')
                results.update_hash_func_patch_details(idx, hash_func_patch_details)
            results.log_milestone('finished patching hash funcs')



        except Exception as e:
            results.add_err(str(e))

    def perform_static_inline_filters(self, discovered_hash_funcs, graph_order_diff_limit):
        lonely_hash_funcs = self.find_lonely_hash_funcs(discovered_hash_funcs)
        passed_hash_funcs = []
        if len(lonely_hash_funcs) > 0:
            passed_hash_funcs = self.filter_hash_funcs_graph_order(lonely_hash_funcs, graph_order_diff_limit)
        return passed_hash_funcs


    def find_lonely_hash_funcs(self, discovered_hash_funcs):
        hash_func_name_to_freq = {}
        for discovered_hash_func in discovered_hash_funcs:
            discovered_hash_func_name = discovered_hash_func.hash_func_name
            if discovered_hash_func_name not in hash_func_name_to_freq:
                hash_func_name_to_freq[discovered_hash_func_name] = 0
            hash_func_name_to_freq[discovered_hash_func_name] += 1

        lonely_hash_func_names = set([name for name in hash_func_name_to_freq if hash_func_name_to_freq[name] == 1])
        return [discovered_hash_func for discovered_hash_func in discovered_hash_funcs if discovered_hash_func.hash_func_name in lonely_hash_func_names]


    def filter_hash_funcs_graph_order(self, discovered_hash_funcs, diff_limit):
        passed_hash_funcs = []
        for discovered_hash_func in discovered_hash_funcs:
            passed_scc_order = len(discovered_hash_func.largest_passed_scc_node_addrs)
            func_graph_order = discovered_hash_func.func_num_blocks
            scc_func_graph_diff = func_graph_order - passed_scc_order
            if diff_limit is None or scc_func_graph_diff <= diff_limit:
                passed_hash_funcs.append(discovered_hash_func)
        return passed_hash_funcs

    def load_hash_vals_checks(self):
        models_file = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'hash_func_to_str_to_hash_vals.json')
        with open(models_file, 'r') as fd:
            return json.load(fd)


    def hash_func_is_used(self, hash_func_use):
        hash_func_reads = hash_func_use.reads
        hash_func_writes = hash_func_use.writes
        hash_func_seeks = hash_func_use.seeks

        return len(hash_func_reads) > 0 or len(hash_func_writes) > 0 or len(hash_func_seeks) > 0

class HashFuncPatchingAnalysisResults(AnalysisResults,MilestoneLogger):

    def __init__(self):
        MilestoneLogger.__init__(self)
        AnalysisResults.__init__(self)
        self.hash_funcs_discovery_analysis_results = None
        self.hash_funcs_to_patch = None
        self.hash_func_use_analysis_results = None
        self.hash_func_patch_details_list = []
        self.errs = []

    def set_hash_funcs_discovery_analysis_results(self, value):
        self.hash_funcs_discovery_analysis_results = value

    def set_hash_funcs_to_patch(self, value):
        self.hash_funcs_to_patch = value

    def set_hash_func_use_analysis_results(self, value):
        self.hash_func_use_analysis_results = value

    def add_hash_func_patch_details(self, hash_func_patch_details):
        self.hash_func_patch_details_list.append(hash_func_patch_details)
        return len(self.hash_func_patch_details_list) - 1

    def update_hash_func_patch_details(self, i, hash_func_patch_details):
        self.hash_func_patch_details_list[i] = hash_func_patch_details


    def get_tracked_events(self):
        tracked_events = {}
        tracked_events['errs'] = len(self.errs)
        tracked_events['timeouts'] = len([e for e in self.errs if 'timeout' in e])

        return tracked_events

    def copy_from_inner(self, other_analysis_results):
        self.set_hash_funcs_discovery_analysis_results(other_analysis_results.hash_funcs_discovery_analysis_results)
        self.set_hash_funcs_to_patch(other_analysis_results.hash_funcs_to_patch)
        self.set_hash_func_use_analysis_results(other_analysis_results.hash_func_use_analysis_results)
        for hash_func_patch_details in other_analysis_results.hash_func_patch_details_list:
            self.add_hash_func_patch_details(hash_func_patch_details)
        for milestone_timestamp in other_analysis_results.milestone_timestamps:
            milestone_msg, milestone_time = milestone_timestamp
            self.add_milestone(milestone_msg, milestone_time)
        for err in other_analysis_results.errs:
            self.add_err(err)

class HashFuncPatchDetails(MilestoneLogger):

    # patch_target should be  discovered hash func
    def __init__(self, patch_target):
        MilestoneLogger.__init__(self)
        self.patch_target = patch_target

        self.orig_hash_symb_ret_irregularities = None
        self.orig_case_sensitive = None
        self.orig_hash_val_irregularities = None
        self.orig_hash_sig1 = None
        self.orig_hash_sig2 = None
        self.orig_hash_ext_mem_ref_irregularities = None

        self.patch_insert_successes = None

        self.patch_case_sensitive = None
        self.patch_hash_ext_mem_ref_irregularities = None

        self.range_irregularities = None

        self.milestone_timestamps = []

        self.errs = []

    def add_err(self, err):
        self.errs.append(err)

    def any_irregularities(self):
        if self.orig_hash_symb_ret_irregularities is None or len(self.orig_hash_symb_ret_irregularities) > 0:
            return True
        if self.orig_hash_val_irregularities is None or len(self.orig_hash_val_irregularities) > 0:
            return True
        if self.orig_hash_ext_mem_ref_irregularities is None or len(self.orig_hash_ext_mem_ref_irregularities) > 0:
            return True
        if self.patch_hash_ext_mem_ref_irregularities is None or len(self.patch_hash_ext_mem_ref_irregularities) > 0:
            return True
        if self.range_irregularities is None or len(self.range_irregularities) > 0:
            return True
        return False


