import jsonpickle

from discovered_hash_func_key import DiscoveredHashFuncKey
from analyses.hash_func_patching.hash_func_patching_analysis import HashFuncPatchingAnalysisResults
from analyses.hash_func_discovery.hash_func_discovery_analysis import DiscoveredFunc


def map_md5_to_file_names(results_list):
    md5_to_file_names = {}
    for results in results_list:
        analysis_results = results.analysis_results
        md5 = results.analysis_target.file_md5
        file_name = results.analysis_target.full_file_path
        if md5 not in md5_to_file_names:
            md5_to_file_names[md5] = set()
        md5_to_file_names[md5].add(file_name)
    return md5_to_file_names

def map_md5_to_analysis_errs(results_list):
    md5_to_errs = {}
    for results in results_list:
        analysis_results = results.analysis_results
        all_errs = get_all_errs(analysis_results)
        if len(all_errs) > 0:
            md5 = results.analysis_target.file_md5
            assert md5 not in md5_to_errs
            md5_to_errs[md5] = all_errs
    return md5_to_errs

def map_analysis_err_to_md5s(md5_to_errs):
    err_to_md5s = {}
    for md5, errs in md5_to_errs.items():
        for err in errs:
            if err not in err_to_md5s:
                err_to_md5s[err] = set()
            err_to_md5s[err].add(md5)
    return err_to_md5s


def map_dhf_key_to_dhf(results_list):
    dhf_key_to_dhf = {}
    for results in results_list:
        analysis_results = results.analysis_results
        analysis_target = results.analysis_target
        hash_func_discovery_analysis_results = get_hash_func_discovery_analysis_results(analysis_results)
        discovered_hash_funcs = hash_func_discovery_analysis_results.discovered_hash_funcs
        for discovered_hash_func in discovered_hash_funcs:
            dhf_key = DiscoveredHashFuncKey(analysis_target, discovered_hash_func)
            assert dhf_key not in dhf_key_to_dhf, "We have duplicate keys: {}".format(dhf_key)
            dhf_key_to_dhf[dhf_key] = discovered_hash_func
    return dhf_key_to_dhf


def discard_results_with_analysis_errs(results_list):
    new_results_list = []
    for results in results_list:
        analysis_results = results.analysis_results
        all_errs = get_all_errs(analysis_results)
        if len(all_errs) == 0:
            new_results_list.append(results)
    return new_results_list


def extract_patch_target_dhf_keys(results_list):
    patch_target_dhf_keys = set()
    for results in results_list:
        analysis_results = results.analysis_results
        analysis_target = results.analysis_target

        hash_funcs_to_patch = analysis_results.hash_funcs_to_patch
        if hash_funcs_to_patch is None:
            continue
        for hash_func_to_patch in hash_funcs_to_patch:
            assert type(hash_func_to_patch).__name__ == "DiscoveredFunc"
            dhf_key = DiscoveredHashFuncKey(analysis_target, hash_func_to_patch)
            patch_target_dhf_keys.add(dhf_key)


    return patch_target_dhf_keys

def extract_analysis_time_info(results_list):
    min_time, max_time, total_time, ave_time = float('inf'), -float('inf'), 0.0, 0.0
    min_time_file, min_time_md5, max_time_file, max_time_md5 = None, None, None, None
    num = 0
    for results in results_list:
        analysis_target = results.analysis_target
        file_path = analysis_target.full_file_path
        file_md5 = analysis_target.file_md5
        analysis_results = results.analysis_results
        start_time = analysis_results.start_time
        end_time = analysis_results.end_time
        run_time = end_time - start_time
        total_seconds = run_time.total_seconds()
        if total_seconds < min_time:
            min_time = total_seconds
            min_time_file = file_path
            min_time_md5 = file_md5
        if total_seconds > max_time:
            max_time = total_seconds
            max_time_file = file_path
            max_time_md5 = file_md5
        total_time += total_seconds
        num += 1

    ave_time = total_time / num


    return min_time, min_time_file, min_time_md5, max_time, max_time_file, max_time_md5, total_time, ave_time

def map_md5_to_hash_alg_to_dhfs(dhf_key_to_dhf, hash_algs):
    md5_to_dhf_key_to_dhf = group_by_md5(dhf_key_to_dhf)
    md5_to_hash_alg_to_dhfs = {}
    for md5, dhf_key_to_dhf in md5_to_dhf_key_to_dhf.items():
        hash_alg_to_dhf_key_to_dhf = group_by_hash_alg(dhf_key_to_dhf, hash_algs)
        assert md5 not in md5_to_hash_alg_to_dhfs
        md5_to_hash_alg_to_dhfs[md5] = hash_alg_to_dhf_key_to_dhf
    return md5_to_hash_alg_to_dhfs

def extract_lone_dhfs(md5_to_hash_alg_to_dhfs):
    lone_dhf_key_to_dhf = {}
    for md5, hash_alg_to_dhfs in md5_to_hash_alg_to_dhfs.items():
        for hash_alg, dhf_key_to_dhf in hash_alg_to_dhfs.items():
            # There is only one discovered hash func of this hash alg, therefore it is a lone hash func
            if len(dhf_key_to_dhf) == 1:
                dhf_key, dhf = list(dhf_key_to_dhf.items())[0]
                assert dhf_key not in lone_dhf_key_to_dhf
                lone_dhf_key_to_dhf[dhf_key] = dhf
    return lone_dhf_key_to_dhf

# Map the number of hash functions in a binary to the number of binaries with this number of hash functions
def map_num_dhfs_to_num_md5s(dhf_key_to_dhf):
    md5_to_dhf_key_to_dhf = group_by_md5(dhf_key_to_dhf)
    num_dhfs_to_num_md5s = {}

    for md5, dhf_key_to_dhf in md5_to_dhf_key_to_dhf.items():
        num_dhfs = len(dhf_key_to_dhf)
        if num_dhfs not in num_dhfs_to_num_md5s:
            num_dhfs_to_num_md5s[num_dhfs] = 0
        num_dhfs_to_num_md5s[num_dhfs] += 1
    return num_dhfs_to_num_md5s
            


    

                
def group_by_hash_alg(dhf_key_to_something, hash_algs):
    hash_alg_to_something = {hash_alg:{} for hash_alg in hash_algs}
    for dhf_key, something in dhf_key_to_something.items():
        hash_alg = dhf_key.hash_alg
        if hash_alg in hash_alg_to_something:
            assert dhf_key not in hash_alg_to_something[hash_alg]
            hash_alg_to_something[hash_alg][dhf_key] = something
    return hash_alg_to_something

def group_by_md5(dhf_key_to_something):
    md5_to_something = {}
    for dhf_key, something in dhf_key_to_something.items():
        md5 = dhf_key.file_md5
        if md5 not in md5_to_something:
            md5_to_something[md5] = {}
        assert dhf_key not in md5_to_something[md5]
        md5_to_something[md5][dhf_key] = something
    return md5_to_something

def discard_dhf_keys_of_errored_analyses(dhf_key_map, md5_to_errs):
    selcted_dhf_key_map = dhf_key_map.copy()
    for dhf_key in dhf_key_map:
        md5 = dhf_key.file_md5
        if md5 in md5_to_errs:
            selcted_dhf_key_map.pop(dhf_key)

    return selcted_dhf_key_map


def read_results(results_file_name):
    results_list = []
    deserialization_errs = 0
    with open(results_file_name, 'r') as fd:
        for i, line in enumerate(fd):
            try:
                results = jsonpickle.decode(line)
                results_list.append(results)
            except Exception as e:
                deserialization_errs += 1
    return results_list, deserialization_errs

def map_analysis_target_md5_to_results(all_results):
    analysis_target_md5_to_results = {}
    for results in all_results:
        analysis_target = results.analysis_target
        md5 = analysis_target.file_md5
        assert md5 not in analysis_target_md5_to_results, "We have duplicate MD5s: {}".format(md5)
        analysis_target_md5_to_results[md5] = results
    return analysis_target_md5_to_results

def get_results_with_analysis_errs(dhf_key_to_results):
    dhf_with_analysis_errs = {}
    for dhf_key, results_list in dhf_key_to_results.items():
        for analysis_target, analysis_results in results_list:
            all_errs = get_all_errs(analysis_results)
            if len(all_errs) > 0:
                dhf_with_analysis_errs[dhf_key] = results_list
                break
    return dhf_with_analysis_errs

def get_all_errs(analysis_results):
    hash_func_discovery_analysis_results = get_hash_func_discovery_analysis_results(analysis_results)
    return analysis_results.errs + hash_func_discovery_analysis_results.errs

def map_err_to_dhf(dhf_to_results):
    err_to_dhf = {}
    for dhf, results_list in dhf_to_results.items():
        for analysis_target, analysis_results in results_list:
            errs = get_all_errs(analysis_results)
            for err in errs:
                if err not in err_to_dhf:
                    err_to_dhf[err] = set()
                err_to_dhf[err].add(dhf)

    return err_to_dhf

def map_dhf_key_to_patch_details(results_list):
    dhf_key_to_patch_details = {}
    for results in results_list:
        analysis_target = results.analysis_target
        analysis_results = results.analysis_results
        assert type(analysis_results).__name__ == "HashFuncPatchingAnalysisResults", "Not patch analysis results!"
        for hash_func_patch_details in analysis_results.hash_func_patch_details_list:
            discovered_hash_func = hash_func_patch_details.patch_target
            dhf_key = DiscoveredHashFuncKey(analysis_target, discovered_hash_func)
            assert dhf_key not in dhf_key_to_patch_details, "We already have patch details for {}".format(dhf_key)
            dhf_key_to_patch_details[dhf_key] = hash_func_patch_details
    return dhf_key_to_patch_details


def classify_patch_details(dhf_key_to_patch_details, md5_to_errs):
    dhf_key_to_patched = {}
    dhf_key_to_timeouts = {}
    dhf_key_to_exceptions = {}
    #dhf_key_to_errs = {}
    dhf_key_to_sig_errs = {}
    dhf_key_to_space_errs = {}
    dhf_key_to_io_irrs = {}
    dhf_key_to_mem_irrs = {}
    dhf_key_to_patch_irrs = {}

    for dhf_key, patch_details in dhf_key_to_patch_details.items():
        dhf = patch_details.patch_target
        analysis_errs = md5_to_errs[dhf_key.file_md5] if dhf_key.file_md5 in md5_to_errs else []
        orig_hash_symb_ret_irregularities = patch_details.orig_hash_symb_ret_irregularities
        orig_hash_val_irregularities = patch_details.orig_hash_val_irregularities
        orig_hash_ext_mem_ref_irregularities = patch_details.orig_hash_ext_mem_ref_irregularities
        patch_hash_ext_mem_ref_irregularities = patch_details.patch_hash_ext_mem_ref_irregularities
        range_irregularities = patch_details.range_irregularities
        patch_errs = patch_details.errs
        milestone_timestamps = patch_details.milestone_timestamps
        last_milestone_label = milestone_timestamps[-1][0]
        
        # Successful patches
        if (orig_hash_symb_ret_irregularities is not None and len(orig_hash_symb_ret_irregularities) == 0) and \
                (orig_hash_val_irregularities is not None and len(orig_hash_val_irregularities) == 0) and \
                (orig_hash_ext_mem_ref_irregularities is not None and len(orig_hash_ext_mem_ref_irregularities) == 0) and \
                (patch_hash_ext_mem_ref_irregularities is not None and len(patch_hash_ext_mem_ref_irregularities) == 0) and \
                (range_irregularities is not None and len(range_irregularities) == 0) and \
                last_milestone_label == 'completed: patch evaluation finished':
            assert dhf_key not in dhf_key_to_patched, "We have duplicate keys: {}".format(dhf_key)
            dhf_key_to_patched[dhf_key] = dhf
            continue

        if any('timeout' in err.lower() for err in analysis_errs):
            if last_milestone_label != 'completed: patch evaluation finished':
                assert dhf_key not in dhf_key_to_timeouts, "We have duplicate keys: {}".format(dhf_key)
                dhf_key_to_timeouts[dhf_key] = dhf
                continue

        if len(analysis_errs) > 0:
            assert dhf_key not in dhf_key_to_exceptions, "We have duplicate keys: {}".format(dhf_key)
            dhf_key_to_exceptions[dhf_key] = dhf
            continue

        if len(patch_errs) > 0:
            if "Could not find a signature that works (aborting patching)." in patch_errs:
                assert dhf_key not in dhf_key_to_sig_errs, "We have duplicate keys: {}".format(dhf_key)
                dhf_key_to_sig_errs[dhf_key] = dhf
            elif "Could not add replacement hash function to binary (aborting patching)." in patch_errs:
                assert dhf_key not in dhf_key_to_space_errs, "We have duplicate keys: {}".format(dhf_key)
                dhf_key_to_space_errs[dhf_key] = dhf
            else:
                raise Exception("Unhandled patch errs: {}".format(patch_errs))
            continue
        if (orig_hash_symb_ret_irregularities is not None and len(orig_hash_symb_ret_irregularities) > 0) or (orig_hash_val_irregularities is not None and len(orig_hash_val_irregularities) > 0):
            assert dhf_key not in dhf_key_to_io_irrs, "We have duplicate keys: {}".format(dhf_key)
            dhf_key_to_io_irrs[dhf_key] = dhf
            continue

        if (orig_hash_ext_mem_ref_irregularities is not None and len(orig_hash_ext_mem_ref_irregularities) > 0):
            assert dhf_key not in dhf_key_to_mem_irrs, "We have duplicate keys: {}".format(dhf_key)
            dhf_key_to_mem_irrs[dhf_key] = dhf
            continue

        if (patch_hash_ext_mem_ref_irregularities is not None and len(patch_hash_ext_mem_ref_irregularities) > 0) or \
            (range_irregularities is not None and len(range_irregularities) > 0):
            assert dhf_key not in dhf_key_to_patch_irrs, "We have duplicate keys: {}".format(dhf_key)
            dhf_key_to_patch_irrs[dhf_key] = dhf
            continue
    
        raise Exception("Unclassified patch details: {}".format(dhf_key))
    assert set(dhf_key_to_patched) | \
        set(dhf_key_to_timeouts) | \
        set(dhf_key_to_exceptions) | \
        set(dhf_key_to_sig_errs) | \
        set(dhf_key_to_space_errs) | \
        set(dhf_key_to_io_irrs) | \
        set(dhf_key_to_mem_irrs) | \
        set(dhf_key_to_patch_irrs) == set(dhf_key_to_patch_details)
    return dhf_key_to_patched, dhf_key_to_timeouts, dhf_key_to_exceptions, dhf_key_to_sig_errs, dhf_key_to_space_errs, dhf_key_to_io_irrs, dhf_key_to_mem_irrs, dhf_key_to_patch_irrs

def get_discovered_hash_funcs(analysis_results):
    hash_func_discovery_analysis_results = get_hash_func_discovery_analysis_results(analysis_results)
    return hash_func_discovery_analysis_results.discovered_hash_funcs

def get_hash_func_discovery_analysis_results(analysis_results):
    if type(analysis_results).__name__ == "HashFuncPatchingAnalysisResults":
        hash_func_discovery_analysis_results =  analysis_results.hash_funcs_discovery_analysis_results
    elif type(analysis_results).__name__ == "HashFuncDiscoveryAnalysisResults":
        hash_func_discovery_analysis_results = analysis_results
    else:
        raise Exception("I don't know where to get the HashFuncDiscoveryAnalysisResults in type: {}: {}".format(type(analysis_results), analysis_results))
    return hash_func_discovery_analysis_results
