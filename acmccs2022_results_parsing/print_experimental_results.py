import argparse
import sys
import os
import matplotlib.pyplot as plt

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import parsing
import ground_truth_parsing
import dict_util

#hash_algs = sorted(['aphash', 'bkdrhash', 'dekhash', 'djbhash', 'elfhash', 'fnvhash', 'jshash', 'rshash', 'sdbmhash'])
hash_algs = sorted(['bkdrhash', 'dekhash', 'djbhash', 'elfhash', 'fnvhash', 'jshash', 'rshash', 'sdbmhash'])

def main():

    parser = argparse.ArgumentParser()
    parser.add_argument("--results", required=True, help='the file with the first results')
    parser.add_argument('--discovery_only', action='store_true', default=False, help='Only print discovery results')
    parser.add_argument('--summaries_only', action='store_true', default=False, help='Only print totals')
    parser.add_argument('--gt_only', help='Only consider hash funcs in ground truth data set')
    parser.add_argument('--discard_errored', action='store_true', default=False, help='Remove all analysis targets that have any errors in either results from consideration')
    parser.add_argument('--include_hash_funcs_per_binary', action='store_true', default=False, help='Include information on the number of hash functions per binary (will draw a graph)')

    args = parser.parse_args()
    results_file = args.results
    discovery_only = args.discovery_only
    summaries_only = args.summaries_only
    gt_file = args.gt_only
    discard_errored = args.discard_errored
    include_hash_funcs_per_binary = args.include_hash_funcs_per_binary
    print("WORKING ON FILE: {}".format(results_file))

    results_file_base_name = os.path.basename(results_file)

    all_results, deserialization_errs = parsing.read_results(results_file)
    print("Num results: {}".format(len(all_results)))
    print("Deserialization errors: {}".format(deserialization_errs))


    md5_to_errs = parsing.map_md5_to_analysis_errs(all_results)
    err_to_md5s = parsing.map_analysis_err_to_md5s(md5_to_errs)


    selected_results = all_results
    if discard_errored:
        selected_results = parsing.discard_results_with_analysis_errs(selected_results)


    dhf_key_to_dhf = parsing.map_dhf_key_to_dhf(selected_results)

    md5_to_file_names = parsing.map_md5_to_file_names(selected_results)

    min_time, min_time_file, min_time_md5, max_time, max_time_file, max_time_md5, tot_time, ave_time = parsing.extract_analysis_time_info(selected_results)
    print("Min time: {:0.03f}s for {} ({})".format(min_time, min_time_file, min_time_md5))
    print("Max time: {:0.03f}s for {} ({})".format(max_time, max_time_file, max_time_md5))
    print("Tot time: {:0.03f}s".format(tot_time))
    print("Ave time: {:0.03f}s".format(ave_time))
    print("DISCOVERY:")


    selected_dhf_key_to_dhf = dhf_key_to_dhf
    selected_hash_algs = hash_algs

    # If gt_file is not None, we assume we are only interested in the hash functions included in the ground truth set
    if gt_file is not None:
        ground_truth_set = ground_truth_parsing.get_ground_truth_set(gt_file)

        # Update hash_algs to only include those in the GT set
        ground_truth_hash_algs = ground_truth_parsing.extract_ground_truth_hash_algs(ground_truth_set)
        selected_hash_algs = sorted(list(ground_truth_hash_algs))

        # Recalculate errors to include only binaries in GT set
        ground_truth_md5s = ground_truth_parsing.extract_ground_truth_md5s(ground_truth_set)
        md5_to_errs = dict_util.dict_intersect_set(md5_to_errs, ground_truth_md5s)
        err_to_md5s = parsing.map_analysis_err_to_md5s(md5_to_errs)

        ground_truth_parsing.print_gthf_key_to_gthfs(ground_truth_set, "GT SET", summaries_only, md5_to_file_names, selected_hash_algs)

        missed_gthf_key_to_dhf = dict_util.dict_remove_set(ground_truth_set, selected_dhf_key_to_dhf)
        # print the hash functions in the GT set that were not discovered
        ground_truth_parsing.print_gthf_key_to_gthfs(missed_gthf_key_to_dhf, "GT MISSED", summaries_only, md5_to_file_names, selected_hash_algs)

        extra_gthf_key_to_dhf = dict_util.dict_remove_set(selected_dhf_key_to_dhf, ground_truth_set)
        # print the hash functions that were discovered that are not in the GT set
        print_dhf_key_to_dhf_by_alg(extra_gthf_key_to_dhf, "GT EXTRA", summaries_only, md5_to_file_names, selected_hash_algs)

        selected_dhf_key_to_dhf = dict_util.dict_intersect_set(selected_dhf_key_to_dhf, ground_truth_set.keys())

        
    # print
    print_dhf_key_to_dhf_by_alg(selected_dhf_key_to_dhf, "DISCOVERED (Candidate)", summaries_only, md5_to_file_names, selected_hash_algs) 

    print("ERRS: {}".format(sum(len(md5s) for md5s in err_to_md5s.values())))
    for err, md5s in err_to_md5s.items():
        print("    {}: {}".format(err, len(md5s)))
        if summaries_only:
            continue
        for md5 in sorted(md5s):
            print("        {}".format(md5))

    md5_to_hash_alg_to_dhfs = parsing.map_md5_to_hash_alg_to_dhfs(selected_dhf_key_to_dhf, selected_hash_algs)
    # Info on number of hash functions per binary
    if include_hash_funcs_per_binary:
        print("Number of binaries with at least one hash func: {}".format(len(md5_to_hash_alg_to_dhfs)))
        num_dhfs_to_num_md5s = parsing.map_num_dhfs_to_num_md5s(dhf_key_to_dhf)
        for num_dhfs, num_md5s in sorted(num_dhfs_to_num_md5s.items(), key=lambda x: x[0]):
            print("Number of binaries with {} hash functions: {}".format(num_dhfs, num_md5s))
        draw_num_dfhs_to_num_md5s(num_dhfs_to_num_md5s, results_file_base_name)

    if discovery_only:
        return


    lone_dhf_key_to_dhfs = parsing.extract_lone_dhfs(md5_to_hash_alg_to_dhfs)
    # print
    print_dhf_key_to_dhf_by_alg(lone_dhf_key_to_dhfs, "LONE", summaries_only, md5_to_file_names, selected_hash_algs) 

    print("PATCHING:")

    patch_target_dhf_keys = parsing.extract_patch_target_dhf_keys(selected_results)
    patch_target_dhf = dict_util.dict_intersect_set(selected_dhf_key_to_dhf, patch_target_dhf_keys)
    removed_dhf = dict_util.dict_remove_set(selected_dhf_key_to_dhf, patch_target_dhf_keys)

    

    # print
    print_dhf_key_to_dhf_by_alg(removed_dhf, "FILTER OUT", summaries_only, md5_to_file_names, selected_hash_algs)

    # print
    print_dhf_key_to_dhf_by_alg(patch_target_dhf, "FILTER IN (Isolated)", summaries_only, md5_to_file_names, selected_hash_algs)

    dhf_key_to_patch_details = parsing.map_dhf_key_to_patch_details(selected_results) 
    if gt_file is not None: 
        # Reduce to only those hash functions in GT set
        dhf_key_to_patch_details = dict_util.dict_intersect_set(dhf_key_to_patch_details, set(selected_dhf_key_to_dhf))

    patch_target_not_attempted = dict_util.dict_remove_set(patch_target_dhf, set(dhf_key_to_patch_details))
    # print
    print_dhf_key_to_dhf_by_alg(patch_target_not_attempted, "NOT ATTEMPTED DUE TO TIMEOUT", summaries_only, md5_to_file_names, selected_hash_algs)

    dhf_key_to_patched, dhf_key_to_timeouts, dhf_key_to_exceptions, dhf_key_to_sig_errs, dhf_key_to_space_errs, dhf_key_to_io_irrs, dhf_key_to_mem_irrs, dhf_key_to_patch_irrs = parsing.classify_patch_details(dhf_key_to_patch_details, md5_to_errs)




    # print
    print_dhf_key_to_dhf_by_alg(dhf_key_to_patched, "PATCHED", summaries_only, md5_to_file_names, selected_hash_algs)

    # Information on the number of SipHash and UnivHash replacement hash functions
    num_successes = len(dhf_key_to_patched)
    num_sip_successes = 0
    num_univ_successes = 0
    dhf_key_to_successful_patch_details = dict_util.dict_intersect_set(dhf_key_to_patch_details, set(dhf_key_to_patched))
    for dhf_key, patch_details in dhf_key_to_successful_patch_details.items():
        patch_insert_successes = patch_details.patch_insert_successes
        sip_success = any('sip_hash' in h and v for h, v in patch_insert_successes.items())
        univ_success = any('univ_hash' in h and v for h, v in patch_insert_successes.items())
        #sip_success = bool(patch_insert_successes['sip_hash_func.s']) if 'sip_hash_func.s' in patch_insert_successes else False # This does not work becaues we also have sip_hash_func_case_insensitive (and similarly for univ)
        #univ_success = bool(patch_insert_successes['univ_hash_func.s']) if 'univ_hash_func.s' in patch_insert_successes else False
        assert (sip_success or univ_success), "We have a successful patch, but neither SipHash nor UnivHash were inserted successfully? {} {} ({})".format(sip_success, univ_success, dhf_key)
        assert sip_success ^ univ_success, "Exactly one of SipHash and UnivHash should be inserted successfully, but: {} {} ({})".format(sip_success, univ_success, dhf_key)

        if sip_success:
            num_sip_successes += 1
        else:
            num_univ_successes += 1

    assert num_sip_successes + num_univ_successes == num_successes
    print("Number of SipHash replacements: {} ({:.03f}%)".format(num_sip_successes, (num_sip_successes / num_successes) * 100.0))
    print("Number of UniversalHash replacements: {} ({:.03f}%)".format(num_univ_successes, (num_univ_successes / num_successes) * 100.0))

    # print
    print_dhf_key_to_dhf_by_alg(dhf_key_to_sig_errs, "NO PATCH SIG", summaries_only, md5_to_file_names, selected_hash_algs)

    # print
    print_dhf_key_to_dhf_by_alg(dhf_key_to_io_irrs, "NO PATCH IO", summaries_only, md5_to_file_names, selected_hash_algs)

    # print
    print_dhf_key_to_dhf_by_alg(dhf_key_to_mem_irrs, "NO PATCH MEM", summaries_only, md5_to_file_names, selected_hash_algs)

    # print
    print_dhf_key_to_dhf_by_alg(dhf_key_to_space_errs, "NO PATCH SPACE", summaries_only, md5_to_file_names, selected_hash_algs)

    # print
    print_dhf_key_to_dhf_by_alg(dhf_key_to_patch_irrs, "NO PATCH VERIF", summaries_only, md5_to_file_names, selected_hash_algs)

    # print
    print_dhf_key_to_dhf_by_alg(dhf_key_to_timeouts, "NO PATCH T/O", summaries_only, md5_to_file_names, selected_hash_algs)

    # print
    print_dhf_key_to_dhf_by_alg(dhf_key_to_exceptions, "NO PATCH ERROR", summaries_only, md5_to_file_names, selected_hash_algs)
    

def print_dhf_key_to_dhf_by_alg(dhf_key_to_dhf, title, summaries_only, md5_to_file_names, selected_hash_algs):
    hash_alg_to_dhf = parsing.group_by_hash_alg(dhf_key_to_dhf, selected_hash_algs)
    print("{}: {}".format(title, sum(len(dhf_key_to_dhf) for dhf_key_to_dhf in hash_alg_to_dhf.values())))
    for hash_alg in sorted(selected_hash_algs):
        dhf_key_to_dhf = hash_alg_to_dhf[hash_alg]
        print("    {}: {}".format(hash_alg, len(dhf_key_to_dhf)))
        if summaries_only:
            continue
        for i, dhf_key in enumerate(sorted(dhf_key_to_dhf)):
            dhf = dhf_key_to_dhf[dhf_key]
            md5 = dhf_key.file_md5
            file_names = list(sorted(md5_to_file_names[md5]))
            if len(file_names) == 1:
                file_names = file_names[0]
            func_addr = dhf_key.func_addr
            assert hash_alg == dhf_key.hash_alg
            assert dhf.func_addr == func_addr
            print("        {:4}: {}@0x{:x} in [{}] ({}) ({})".format(i + 1, dhf.func_name, func_addr, file_names, md5, hash_alg))

def draw_num_dfhs_to_num_md5s(num_dhfs_to_num_md5s, results_file_base_name):
    bins = sorted(list(num_dhfs_to_num_md5s.keys()))
    ys = [num_dhfs_to_num_md5s[b] for b in bins]
    axes = plt.gca()
    axes.bar(range(len(bins)), ys)
    axes.tick_params(labelsize=12)
    plt.xticks(range(len(bins)), bins, rotation=0)
    for x,y in enumerate(ys):
        plt.text(x, y + 6, str(y), ha='center', fontsize=10, rotation=0)
    plt.xlabel("Number of Candidate Hash Functions in Binary", fontsize=20)
    plt.ylabel("Number of Binaries", fontsize=20)
    plt.show()


if __name__ == "__main__":
    main()
