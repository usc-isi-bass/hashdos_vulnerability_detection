import argparse
import os
import angr
import logging
import multiprocessing
import multiprocessing.dummy as threading
import re
import ntpath
import hashlib
import json
import time
import datetime

import input_dependence_analyses
import hash_func_use_context_analyses
import hash_func_search

logging.getLogger('angr').setLevel(logging.CRITICAL)
logging.getLogger('cle').setLevel(logging.CRITICAL)
logging.getLogger('archinfo').setLevel(logging.CRITICAL)
logging.getLogger('pyvex').setLevel(logging.CRITICAL)
logging.getLogger('pyvex.lifting.libvex').setLevel(logging.CRITICAL)
logging.getLogger('claripy').setLevel(logging.CRITICAL)
logging.getLogger('claripy.balancer').setLevel(logging.CRITICAL)
logging.getLogger('input_dependence_analyses').setLevel(logging.CRITICAL)
logging.getLogger('hash_func_use_context_analyses').setLevel(logging.CRITICAL)
logging.getLogger('angr.procedures.posix.syslog').setLevel(logging.CRITICAL)

l = logging.getLogger(name=__name__)
l.setLevel('CRITICAL')

FILE_PATH = os.path.dirname(os.path.realpath(__file__))
ELF_HEADER = b"\x7fELF"
TIME_FORMAT = "%H:%M:%S, %9A, %d-%m-%Y"

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--root', help='the root directory from where to search for all the elf files')
    parser.add_argument('--elf_list', help='a file containing a list of names of elf files to analyze')
    parser.add_argument('--nthreads', default=1, type=int, help='number of processes to use.')
    parser.add_argument('--resume', default=None, help='the results file from where to pick up the analysis.')
    parser.add_argument('--timeout', default=600, help='the total amount of time (in seconds) to wait for an analysis to finish.')
    parser.add_argument('--break_limit', default=-1, help='an upperbound on the number of elf files to add to the list (before removing duplicates). -1 is no upper bound')
    parser.add_argument('--remove_duplicates', default=True, help='before the analysis starts, go through the found elf files and make sure each elf MD5 only appears once')
    args = parser.parse_args()
    root_dir = args.root
    elf_list_file = args.elf_list
    nthreads = args.nthreads
    full_results_file_name = args.resume
    timeout = int(args.timeout)
    break_limit = args.break_limit
    should_remove_duplicates = args.remove_duplicates
    elf_hash_cache = {}
    if not ((root_dir is not None) ^ (elf_list_file is not None)):
        print("You must specify exactly one of either --root or --elf_list")
        return
    if full_results_file_name:
        print("Rebuilding cache from results file {}".format(full_results_file_name))
        elf_hash_cache = rebuild_cache(full_results_file_name)
        print("Found {} unique elfs in results file".format(len(elf_hash_cache)))
    else:
        results_file_name = "results_{}".format(time.strftime("%Y%m%d_%0H%0M"))
        full_results_file_name = os.path.join(FILE_PATH, 'results', results_file_name)

    #analyze = input_dependence_analyses.analyze_get_just_hash
    #analyze = input_dependence_analyses.intermediate_static_analysis
    analyze = hash_func_use_context_analyses.analyze
    get_hash_funcs = hash_func_search.get_stashed_hash_funcs

    # Creating a list of elf files (we could keep this as an iterator, but then we won't have a total number and we need to iterate through them all now anyway to remove duplicates)
    elf_file_names = []
    if root_dir is not None:
        print("Searching for elf files in: {}".format(root_dir))
        for i, elf_file_name in enumerate(find_elfs(root_dir, break_limit)):
            print("Found: {} elfs".format(i + 1), end='\r', flush=True)
            elf_file_names.append(elf_file_name)
    elif elf_list_file is not None:
        print("Retrieving elf list from: {}".format(elf_list_file))
        elf_file_names = parse_elf_list_file(elf_list_file)
    else:
        raise Exception("No root directory specified for searching and no list given. I don't know where to find the elf files.")

    print("Found: {} elfs".format(len(elf_file_names)))
    # Calculating the MD5 for a large number of files will still be pretty slow. Maybe we should parallelize this as well?
    print("Obtaining info for elf files")
    elf_file_infos = get_elf_file_infos(elf_file_names)
    if should_remove_duplicates:
        print("Removing duplicates from elf file list")
        elf_file_infos = remove_duplicates(elf_file_infos)
    if len(elf_hash_cache) > 0:
        print("Removing cached from elf file list")
        elf_file_infos = remove_cached(elf_file_infos, elf_hash_cache)
    total_elf_files = len(elf_file_infos)
    print("Analyzing {} elf files".format(total_elf_files))
    print("Timeout: {}".format(timeout))

    global_start_time = time.localtime()
    print("Analysis started on: {}".format(time.strftime(TIME_FORMAT, global_start_time)))

    # A queue to keep the results of each analysis process
    results_queue = multiprocessing.Manager().Queue()

    # This thread will read the results from the results_queue and write them to a file to track progress
    track_progress_thread = threading.Process(target=track_progress, args=(total_elf_files, results_queue, full_results_file_name,))
    track_progress_thread.start()

    # The thread pool that will do the analysis for each binary
    pool = threading.Pool(nthreads)
    # Create the args to be passed to wrap_analyze
    args = map(lambda elf_file_name: (elf_file_name, elf_file_infos[elf_file_name], analyze, get_hash_funcs, timeout, results_queue), elf_file_infos)
    pool.map(analysis_thread, args)
    results_queue.put("done", block=True)
    track_progress_thread.join()

    global_end_time = time.localtime()
    global_run_time = format_time_delta(global_start_time, global_end_time)
    print("Finished: {}".format(time.strftime(TIME_FORMAT, global_end_time)))
    print("Total time: {}".format(global_run_time))

def analysis_thread(args):
    #full_file_name = args[0]
    #analyze = args[1]
    #get_hash_funcs = args[2]
    #timeout = args[3]
    #results_queue = args[4]
    full_file_name, elf_file_info, analyze, get_hash_funcs, timeout, results_queue = args

    # A shared dictionary between this process and the spawned one to store the results
    result_dict = multiprocessing.Manager().dict()

    # Spawn a new process for the analysis.
    p = multiprocessing.Process(target=wrap_analyze, args=(full_file_name, analyze, get_hash_funcs, result_dict))
    p.start()
    p.join(timeout=timeout)
    if p.is_alive():
        p.terminate()
        p.join()
        if 'err' in result_dict:
            result_dict['err'].append('timeout')
        else:
            result_dict['err'] = ['timeout']
    # Add the results to the queue for saving. We need to copy result_dict, because the shared dictionary will be deleted once this thread dies.
    results_queue.put((elf_file_info, result_dict.copy()), block=True)


# Basically, just take the return value of analyze(...) and add it to the shared dict
def wrap_analyze(full_elf_file_name, analyze, get_hash_funcs, return_result_dict):
    return_result_dict.update(analyze(full_elf_file_name, get_hash_funcs))

# Print how many elfs have been analyzed and store intermediate results
def track_progress(total_elf_files, results_queue, full_results_file_name):
    hit_counter, err_counter, timeout_counter = 0, 0, 0
    num_completed = 0
    print("Results file: {}".format(full_results_file_name))
    time_str = "{:%d %b %Y %H:%M:%S}".format(datetime.datetime.now())
    print("{}/{} elfs, hits: {}, errs: {} T/Os: {} | {}".format(num_completed, total_elf_files, hit_counter, err_counter, timeout_counter, time_str), end='\r')
    with open(full_results_file_name, 'a', buffering=1) as fd_results:
        while True:
            result = results_queue.get(block=True)
            if result == "done":
                time_str = "{:%d %b %Y %H:%M:%S}".format(datetime.datetime.now())
                print("{}/{} elfs, hits: {}, errs: {} T/Os: {} | {}".format(num_completed, total_elf_files, hit_counter, err_counter, timeout_counter, time_str))
                print("Results saved to: {}".format(full_results_file_name))
                results_queue.task_done()
                break
            else:
                elf_dict, result_dict = result
           
                #print("{}\n".format(json.dumps((elf_dict.copy(), result_dict.copy()), default=json_serialiblify)))
                num_completed += 1
                if 'err' in result_dict:
                    err_counter += 1
                    if 'timeout' in result_dict['err']:
                        timeout_counter += 1
                if 'analysis_metrics' in result_dict and 'hit' in result_dict['analysis_metrics'] and result_dict['analysis_metrics']['hit']:
                    hit_counter += 1
                time_str = "{:%d %b %Y %H:%M:%S}".format(datetime.datetime.now())
                print("{}/{} elfs, hits: {}, errs: {} T/Os: {} | {}".format(num_completed, total_elf_files, hit_counter, err_counter, timeout_counter, time_str), end='\r')
                fd_results.write("{}\n".format(json.dumps((elf_dict.copy(), result_dict.copy()), default=json_serialiblify)))
                results_queue.task_done()



def rebuild_cache(full_results_file_name):
    elf_hash_cache = set()
    with open(full_results_file_name, 'r') as fd:
        for i, line in enumerate(fd):
            elf_md5 = json.loads(line)[0]['elf']['elf_md5']
            elf_hash_cache.add(elf_md5)
    return elf_hash_cache


def parse_elf_list_file(elf_list_file_name):
    elf_file_names = []
    with open(elf_list_file_name, 'r') as fd:
        for i, line in enumerate(fd):
            line = line.strip()
            if line[0] == '#': # Allows us to put comments in elf list file
                continue
            elf_file_name = line
            if os.path.isfile(elf_file_name):
                if is_elf_file(elf_file_name):
                    elf_file_names.append(elf_file_name)
                else:
                    raise Exception("Err in elf list file on line {}: file {} exists, but is not an elf file".format(i, elf_file_name))
            else:
                raise Exception("Err in elf list file on line {}: could not find file: {} (please use absolute paths)".format(i, elf_file_name))
    return elf_file_names

def find_elfs(root_dir, break_limit):
    break_counter = 0
    for r, d, f in os.walk(root_dir):
        for file_name in f:
            full_file_name = os.path.join(r, file_name)
            # We don't want to analyze anything that may be somewhere else
            if not os.path.islink(full_file_name):
                if is_elf_file(full_file_name):
                    break_counter += 1
                    yield full_file_name
            if break_limit >= 0 and break_counter >= break_limit:
                break

        if break_limit >= 0 and break_counter >= break_limit:
            break

def is_elf_file(full_file_name):
     with open(full_file_name, 'rb') as fd:
        header = fd.read(4)
        return header == ELF_HEADER

def get_elf_file_infos(elf_file_names):
    full_file_name_to_info = {}
    for i, full_file_name in enumerate(elf_file_names):
        print("{}/{}".format(i + 1, len(elf_file_names)), end='\r', flush=True)
        elf_info = get_elf_info(full_file_name)
        full_file_name_to_info[full_file_name] = elf_info
    return full_file_name_to_info

# Getting some info about the elf file to analyze
def get_elf_info(full_file_name):
    deb_file_name_m = re.search("{0}(?P<deb_file_name>[^{0}]+[.]deb)".format(os.path.sep), full_file_name)
    if not deb_file_name_m:
        raise Exception("Couldn't find .deb file in path of elf file: {}".format(full_file_name))
    deb_file_name = deb_file_name_m.group("deb_file_name")

    elf_file_md5 = md5_file(full_file_name)
    elf_file_name = ntpath.basename(full_file_name)
    elf_file_size = os.stat(full_file_name).st_size

    elf_dict = {'deb':{}, 'elf':{}}
    elf_dict['deb']['deb_name'] = deb_file_name
    elf_dict['elf']['elf_name'] = elf_file_name
    elf_dict['elf']['elf_md5'] = elf_file_md5
    elf_dict['elf']['elf_size'] = elf_file_size

    return elf_dict

# Go through the infos of the elf files and create a new dictionary where each MD5 for an elf file occurs only once
def remove_duplicates(elf_file_infos):
    elf_file_md5_set = set()
    unique_elf_file_infos = {}
    for i, full_file_name in enumerate(elf_file_infos):
        print("{}/{}: found {} unique elfs".format(i + 1, len(elf_file_infos), len(unique_elf_file_infos)), end='\r', flush=True)
        elf_file_info = elf_file_infos[full_file_name]
        elf_file_md5 = elf_file_info['elf']['elf_md5']
        if elf_file_md5 not in elf_file_md5_set:
            elf_file_md5_set.add(elf_file_md5)
            unique_elf_file_infos[full_file_name] = elf_file_info
    print("{}/{}: found {} unique elfs".format(i + 1, len(elf_file_infos), len(unique_elf_file_infos)))
    return unique_elf_file_infos

# Go through the infos and create a new dictionary that only contains those elf files with MD5s that do not occur in the elf_hash_cache (this does not remove duplicates)
def remove_cached(elf_file_infos, elf_hash_cache):
    uncached_elf_file_infos = {}
    for i, full_file_name in enumerate(elf_file_infos):
        print("{}/{}: found {} uncached elfs".format(i + 1, len(elf_file_infos), len(uncached_elf_file_infos)), end='\r', flush=True)
        elf_file_info = elf_file_infos[full_file_name]
        elf_file_md5 = elf_file_info['elf']['elf_md5']
        if elf_file_md5 not in elf_hash_cache:
            uncached_elf_file_infos[full_file_name] = elf_file_info
    print("{}/{}: found {} uncached elfs".format(i + 1, len(elf_file_infos), len(uncached_elf_file_infos)))
    return uncached_elf_file_infos
            

def format_time_delta(start_time, end_time, short=False):
        start_time_datetime = datetime.datetime.fromtimestamp(time.mktime(start_time))
        end_time_datetime = datetime.datetime.fromtimestamp(time.mktime(end_time))
        time_delta_datetime = end_time_datetime - start_time_datetime
        seconds = int(time_delta_datetime.seconds)
        days, seconds = divmod(seconds, 86400)
        hours, seconds = divmod(seconds, 3600)
        minutes, seconds = divmod(seconds, 60)
        if short:
                return "{0}:{1:02d}:{2:02d}:{3:02d}".format(days, hours, minutes, seconds)
        else:
                return "{0} days, {1} hours, {2:02d} minutes and {3:02d} seconds.".format(days, hours, minutes, seconds)
def json_serialiblify(obj):
    if isinstance(obj, set):
        return list(obj)
    else:
        raise TypeError("I don't know how to JSON serialize {} of type: {}".format(obj, type(obj)))

def md5_file(file_name):
    # I got this from: https://stackoverflow.com/questions/3431825/generating-an-md5-checksum-of-a-file
    hash_md5 = hashlib.md5()
    with open(file_name, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()


if __name__ == "__main__":
    main()
