import argparse
import os
import angr
import logging
import multiprocessing
import multiprocessing.dummy as threading
import re
import ntpath
import hashlib
import json
import time
import datetime

import analyses
import hash_func_search

logging.getLogger('angr').setLevel(logging.CRITICAL)
logging.getLogger('cle').setLevel(logging.CRITICAL)
logging.getLogger('archinfo').setLevel(logging.CRITICAL)
logging.getLogger('pyvex').setLevel(logging.CRITICAL)
logging.getLogger('pyvex.lifting.libvex').setLevel(logging.CRITICAL)
logging.getLogger('claripy').setLevel(logging.CRITICAL)
logging.getLogger('claripy.balancer').setLevel(logging.CRITICAL)

l = logging.getLogger(name=__name__)
l.setLevel('CRITICAL')

FILE_PATH = os.path.dirname(os.path.realpath(__file__))
ELF_HEADER = b"\x7fELF"
TIME_FORMAT = "%H:%M:%S, %9A, %d-%m-%Y"

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('root', help='the root directory from where to search for all the elf files')
    parser.add_argument('--nthreads', default=1, type=int, help='number of processes to use.')
    args = parser.parse_args()
    root_dir = args.root
    nthreads = args.nthreads
    timeout=600
    #break_limit = 3000
    break_limit = -1

    analyze = analyses.analyze_get_just_hash
    get_hash_funcs = hash_func_search.get_stashed_hash_funcs

    # Creating a list of elf files (we could keep this as an iterator, but then we won't have a total number)
    print("Searching for elf files in: {}".format(root_dir))
    elf_file_names = []
    for i, elf_file_name in enumerate(find_elfs(root_dir, break_limit)):
        print("Found: {} elfs".format(i + 1), end='\r', flush=True)
        elf_file_names.append(elf_file_name)
    print("Found: {} elfs".format(i + 1))
    total_elf_files = len(elf_file_names)
    print("Analyzing {} elf files".format(total_elf_files))

    global_start_time = time.localtime()
    print("Analysis started on: {}".format(time.strftime(TIME_FORMAT, global_start_time)))

    # A queue to keep the results of each analysis process
    results_queue = multiprocessing.Manager().Queue()

    # This thread will read the results from the results_queue and write them to a file to track progress
    track_progress_thread = threading.Process(target=track_progress, args=(total_elf_files, results_queue,))
    track_progress_thread.start()

    # The thread pool that will do the analysis for each binary
    pool = threading.Pool(nthreads)
    # Create the args to be passed to wrap_analyze
    args = map(lambda elf_file_name: (elf_file_name, analyze, get_hash_funcs, timeout, results_queue), elf_file_names)
    pool.map(analysis_thread, args)
    results_queue.put("done", block=True)
    track_progress_thread.join()

    global_end_time = time.localtime()
    global_run_time = format_time_delta(global_start_time, global_end_time)
    print("Finished: {}".format(time.strftime(TIME_FORMAT, global_end_time)))
    print("Total time: {}".format(global_run_time))

def analysis_thread(args):
    #full_file_name = args[0]
    #analyze = args[1]
    #get_hash_funcs = args[2]
    #timeout = args[3]
    #results_queue = args[4]
    full_file_name, analyze, get_hash_funcs, timeout, results_queue = args

    elf_dict = get_elf_info(full_file_name) 

    # A shared dictionary between this process and the spawned one to store the results
    result_dict = multiprocessing.Manager().dict()

    # Spawn a new process for the analysis.
    p = multiprocessing.Process(target=wrap_analyze, args=(full_file_name, analyze, get_hash_funcs, result_dict))
    p.start()
    p.join(timeout=timeout)
    if p.is_alive():
        p.terminate()
        p.join()
        result_dict['err'] = 'Timeout'
    # Add the results to the queue for saving. We need to copy result_dict, because the shared dictionary will be deleted once this thread dies.
    results_queue.put((elf_dict, result_dict.copy()), block=True)

# Basically, just take the return value of analyze(...) and add it to the shared dict
def wrap_analyze(full_elf_file_name, analyze, get_hash_funcs, return_result_dict):
    return_result_dict.update(analyze(full_elf_file_name, get_hash_funcs))

# Print how many elfs have been analyzed and store intermediate results
def track_progress(total_elf_files, results_queue):
    hit_counter, err_counter, timeout_counter = 0, 0, 0
    num_completed = 0
    results_file_name = "results_{}".format(time.strftime("%Y%m%d_%0H%0M"))
    full_results_file_name = os.path.join(FILE_PATH, 'results', results_file_name)
    print("Results file: {}".format(full_results_file_name))
    with open(full_results_file_name, 'w', buffering=1) as fd_results:
        while True:
            result = results_queue.get(block=True)
            if result == "done":
                print("{}/{} elfs, hits: {}, errs: {} T/Os: {} | {}".format(num_completed, total_elf_files, hit_counter, err_counter, timeout_counter, time_str))
                print("Results saved to: {}".format(full_results_file_name))
                results_queue.task_done()
                break
            else:
                elf_dict, result_dict = result
           
                #print("{}\n".format(json.dumps((elf_dict.copy(), result_dict.copy()), default=json_serialiblify)))
                num_completed += 1
                if 'err' in result_dict:
                    err_counter += 1
                    if result_dict['err'].lower() == 'timeout':
                        timeout_counter += 1
                elif 'analysis_metrics' in result_dict and 'hit' in result_dict['analysis_metrics'] and result_dict['analysis_metrics']['hit']:
                    hit_counter += 1
                time_str = "{:%d %b %Y %H:%M:%S}".format(datetime.datetime.now())
                print("{}/{} elfs, hits: {}, errs: {} T/Os: {} | {}".format(num_completed, total_elf_files, hit_counter, err_counter, timeout_counter, time_str), end='\r')
                fd_results.write("{}\n".format(json.dumps((elf_dict.copy(), result_dict.copy()), default=json_serialiblify)))
                results_queue.task_done()

def find_elfs(root_dir, break_limit):
    break_counter = 0
    for r, d, f in os.walk(root_dir):
        for file_name in f:
            full_file_name = os.path.join(r, file_name)
            # We don't want to analyze anything that may be somewhere else
            if not os.path.islink(full_file_name):
                with open(full_file_name, 'rb') as fd:
                    header = fd.read(4)
                    if header == ELF_HEADER:
                        break_counter += 1
                        yield full_file_name
            if break_limit >= 0 and break_counter >= break_limit:
                break

        if break_limit >= 0 and break_counter >= break_limit:
            break

# Getting some info about the elf file to analyze
def get_elf_info(full_file_name):
    deb_file_name_m = re.search("{0}(?P<deb_file_name>[^{0}]+[.]deb)".format(os.path.sep), full_file_name)
    if not deb_file_name_m:
        raise Exception("Couldn't find .deb file in path of elf file: {}".format(full_file_name))
    deb_file_name = deb_file_name_m.group("deb_file_name")

    elf_file_md5 = md5_file(full_file_name)
    elf_file_name = ntpath.basename(full_file_name)
    elf_file_size = os.stat(full_file_name).st_size

    elf_dict = {'deb':{}, 'elf':{}}
    elf_dict['deb']['deb_name'] = deb_file_name
    elf_dict['elf']['elf_name'] = elf_file_name
    elf_dict['elf']['elf_md5'] = elf_file_md5
    elf_dict['elf']['elf_size'] = elf_file_size

    return elf_dict


def format_time_delta(start_time, end_time, short=False):
        start_time_datetime = datetime.datetime.fromtimestamp(time.mktime(start_time))
        end_time_datetime = datetime.datetime.fromtimestamp(time.mktime(end_time))
        time_delta_datetime = end_time_datetime - start_time_datetime
        seconds = int(time_delta_datetime.seconds)
        days, seconds = divmod(seconds, 86400)
        hours, seconds = divmod(seconds, 3600)
        minutes, seconds = divmod(seconds, 60)
        if short:
                return "{0}:{1:02d}:{2:02d}:{3:02d}".format(days, hours, minutes, seconds)
        else:
                return "{0} days, {1} hours, {2:02d} minutes and {3:02d} seconds.".format(days, hours, minutes, seconds)
def json_serialiblify(obj):
    if isinstance(obj, set):
        return list(obj)
    else:
        raise TypeError

def md5_file(file_name):
    # I got this from: https://stackoverflow.com/questions/3431825/generating-an-md5-checksum-of-a-file
    hash_md5 = hashlib.md5()
    with open(file_name, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()


if __name__ == "__main__":
    main()
